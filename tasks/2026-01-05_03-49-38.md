## 1) Critical issues (bugs / correctness / architectural)

### A. “Half-life” math is wrong (and the tests/docs are also wrong)

Right now you compute:

```rust
decay_factor = exp(-days_since_use / half_life_days)
```

That is **not** a half-life. With that formula, after `half_life_days`, the factor is `e^-1 ≈ 0.368`, not `0.5`.

If you want *actual* half-life semantics (“score halves every N days”), you need:

* `decay_factor = 2^(-days_since_use / half_life_days)` **or**
* `decay_factor = exp(-ln(2) * days_since_use / half_life_days)`

Your comments say “~50% after 7 days,” but your code and test encode “~36.8% after 7 days.” Pick one and align **code + tests + docs**.

This matters because user-facing config and the “why did ordering change?” story depends on it.

---

### B. `record_use()` ignores configured half-life (serious correctness bug)

You load and store `self.half_life_days` in `FrecencyStore`, and you use it on `load()` to recompute scores.

But when you call `record_use()` you do:

```rust
entry.record_use(); // internally uses DEFAULT_SUGGESTED_HALF_LIFE_DAYS
```

So if a user sets a custom half-life, you end up with a mixed dataset:

* everything loaded gets recalculated with configured half-life,
* the moment an item is used, its score is recalculated using the **default** half-life.

That makes sorting inconsistent and “randomly wrong.”

---

### C. Your “frecency” model is not actually frecency (and it causes “rich get richer”)

Current model: `score = total_count * decay(time_since_last_use)`

That has two big consequences:

1. **Frequency never decays.** A script used 500 times last year still carries “500” forever.
2. A single recent use “resurrects” all historical usage. If something was used a ton in the past and you run it once today, it jumps back to ~`count`.

That’s exactly the “rich get richer” problem you called out, and it’s baked into the formula.

Real frecency typically decays *each usage contribution* over time. The common incremental form is:

* Maintain `score` as a float
* On each use, decay the existing score by elapsed time, then add 1:

> `score = score * decay(Δt) + 1`

This gives you intuitive behavior:

* If you stop using something, its score naturally falls.
* Old heavy usage doesn’t dominate forever unless you keep using it.

---

### D. Scores go stale while the app runs

You only recalculate scores on:

* `load()`
* `record_use()`
* `set_half_life_days()`

But time passes continuously. That means the ordering you show is not truly “recency-weighted” unless you refresh scores periodically or compute “score at now” on demand.

If your launcher stays running for hours/days, ranking should drift gradually without requiring a restart or a record_use call.

---

### E. `#[derive(Clone)]` on `FrecencyStore` is dangerous in a multi-window app

`FrecencyStore` contains the entire map and dirty flag. Deriving `Clone` means it’s easy to accidentally create **two independent stores**:

* both mutate independently
* both can save
* last write wins ⇒ silent data loss

In GPUI/multi-window contexts, accidental clones happen more often than people think.

If you need to share it, use an explicit shared owner (`Arc<Mutex<_>>` / actor pattern) and make that “sharing” a deliberate design choice. If you *don’t* need to share, remove `Clone` so the compiler prevents footguns.

---

### F. Persistence is not atomic (risk: corrupted JSON)

`std::fs::write(path, json)` can leave a partially-written file if the app crashes mid-write (or power loss). Next load fails JSON parse and you lose all frecency until user deletes the file.

For user-facing data, you should do “write temp + rename” (atomic replace on Unix).

---

### G. Tie ordering is unstable / jittery

When two scores compare equal (or become equal due to float rounding / underflow), ordering falls back to whatever order the `HashMap` iteration produced, which can change between runs.

If you want the list to feel stable, add deterministic tie-breakers (e.g., `last_used desc`, then `path asc`).

---

## 2) Performance concerns (UI hitch risks, inefficiencies)

### A. `save()` clones the entire map + pretty-prints JSON on the calling thread

This line is expensive for big maps:

```rust
entries: self.entries.clone()
```

Then `to_string_pretty` allocates a big string and formats it.

If `save()` is called on the UI thread (very likely in a GPUI app unless you’re careful), you will hitch. This directly conflicts with “launcher must appear instantly.”

Even if `save()` is called rarely, it’s exactly the kind of thing users feel as “sometimes it stutters.”

**Minimum**: don’t clone; serialize by reference.
**Better**: debounce and save in the background.
**Also**: don’t pretty-print; compact JSON is much faster and smaller.

---

### B. `get_recent_items()` sorts everything even when you only need top N

For thousands of scripts, `O(n log n)` sort is usually still fine, but if you call it frequently (e.g., per keystroke / per render), it can become noticeable.

Use `select_nth_unstable_by` or a small heap to get top N in `O(n)` or `O(n log N)`.

---

### C. Timestamp precision is seconds (probably OK, but it’s a quality loss)

You called this out: multiple uses in the same second can’t be distinguished.

If you move to the incremental score update (`score = score * decay + 1`), second precision is still “fine enough,” but milliseconds are cheap and avoid weirdness with “spammy runs” or testing.

---

## 3) API design feedback (better patterns / abstractions)

### A. Inject “now” for testability and determinism

Right now every score computation calls `SystemTime::now()` internally. That makes correctness tests harder and forces sleeps.

A simple improvement: pass `now` into functions that need it:

* `calculate_score(count, last_used, now, half_life)`
* or give `FrecencyStore` a `clock: impl Fn() -> u64` in tests

This eliminates flaky timing tests and makes time-based behavior much easier to reason about.

---

### B. Add a “revision” counter for cache invalidation

To fix the “filter/group cache not invalidated” issue without “re-filter everything”:

* Maintain `revision: u64` in `FrecencyStore`
* Increment it on any change affecting ranking (`record_use`, remove, clear, half-life change)
* Expose `fn revision(&self) -> u64`
* Any cached derived list stores the revision it was built with; if mismatch, it can:

  * just resort (cheap) if the membership is unchanged
  * or rebuild if necessary

This gives you a simple, reliable invalidation mechanism.

If you want to be fancy, you can also return a delta from `record_use()` (which item changed + new score) so caches can update incrementally.

---

### C. Separate “stored data” vs “derived data”

Right now `score` is both stored and derived (but then always recalculated on load anyway). That’s confusing and adds work.

A clean split:

* Persist: `count`, `last_used`, maybe `raw_score` if using incremental method
* Derive at runtime: `score_at(now)`

---

## 4) Simplification opportunities (reduce complexity without losing capability)

### A. Stop serializing `score`

If you keep your current approach (or even with the improved one), you don’t need `score` in JSON **unless** you adopt the “incremental decayed score” model and persist that raw score.

Right now you recalculate scores on load, so persisting `score` is redundant.

At minimum:

```rust
#[serde(skip_serializing, default)]
pub score: f64,
```

That preserves backwards compatibility when reading old files that contain `score`, but stops writing it going forward.

---

### B. Collapse the three score methods

You currently have:

* `recalculate_score()` (hardcoded default half-life)
* `recalculate_score_with_half_life()`
* `record_use()` (hardcoded default half-life via recalc)

This is how the configured half-life bug happened.

Prefer one path:

* `record_use(half_life_days)`
* `recalculate_score(half_life_days)`

No hidden global constant inside entry methods.

---

## 5) Specific recommendations (concrete changes + example code)

Below are two tiers: **minimal fix** (keep your current model) and **recommended fix** (better frecency model + fixes rich-get-richer).

---

### Tier 1: Minimal fix (keep `count * decay(last_used)` model)

#### 5.1 Fix half-life semantics (or rename it)

If you mean *true* half-life:

```rust
fn calculate_score(count: u32, last_used: u64, half_life_days: f64) -> f64 {
    if !(half_life_days > 0.0) {
        return 0.0;
    }

    let now = current_timestamp();
    let seconds_since_use = now.saturating_sub(last_used);
    let days_since_use = seconds_since_use as f64 / SECONDS_PER_DAY;

    // True half-life decay: halves every `half_life_days`
    let decay_factor = 2f64.powf(-days_since_use / half_life_days);
    count as f64 * decay_factor
}
```

Then update tests/docs accordingly:

* After 7 days: expect ~`count * 0.5`, not `count * e^-1`.

If you *prefer* the current faster decay curve, then rename config/field/docs to something like `decay_time_constant_days` and keep the `exp(-t/τ)` math. But don’t call it half-life.

#### 5.2 Fix configured half-life being ignored in `record_use`

Change entry methods to accept half-life:

```rust
impl FrecencyEntry {
    pub fn record_use_with_half_life(&mut self, half_life_days: f64) {
        self.count = self.count.saturating_add(1);
        self.last_used = current_timestamp();
        self.score = calculate_score(self.count, self.last_used, half_life_days);
    }

    pub fn recalculate_score(&mut self, half_life_days: f64) {
        self.score = calculate_score(self.count, self.last_used, half_life_days);
    }
}
```

And in the store:

```rust
pub fn record_use(&mut self, path: &str) {
    let hl = self.half_life_days;

    match self.entries.get_mut(path) {
        Some(entry) => entry.record_use_with_half_life(hl),
        None => {
            let mut entry = FrecencyEntry::new();
            entry.recalculate_score(hl); // optional; new() sets score=1.0 anyway
            self.entries.insert(path.to_string(), entry);
        }
    }

    self.dirty = true;
    // self.revision += 1; // if you add revisioning
}
```

#### 5.3 Make rankings reflect time passing (avoid stale scores)

Option A (cheap): recompute scores on demand inside `get_recent_items`:

```rust
pub fn get_recent_items(&self, limit: usize) -> Vec<(String, f64)> {
    let now = current_timestamp();
    let hl = self.half_life_days;

    let mut items: Vec<_> = self.entries.iter().map(|(path, entry)| {
        let seconds_since_use = now.saturating_sub(entry.last_used);
        let days_since_use = seconds_since_use as f64 / SECONDS_PER_DAY;
        let decay = 2f64.powf(-days_since_use / hl);
        (path.clone(), entry.count as f64 * decay)
    }).collect();

    items.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal)
        .then_with(|| a.0.cmp(&b.0))); // deterministic tie-breaker
    items.truncate(limit);
    items
}
```

Even if you keep `entry.score`, you should consider computing “score at now” when ranking.

---

### Tier 2: Recommended fix (real frecency + solves rich-get-richer)

This is the model I’d ship for a launcher.

#### 5.4 Change the meaning of `score`: store a decayed accumulator

Keep `count` if you want it for analytics, but don’t use it directly for ranking.

**New behavior:**

* `score` represents the accumulated usage signal **as of `last_used`**
* Ranking uses `score_at(now)`

```rust
impl FrecencyEntry {
    /// Decay factor using true half-life semantics
    fn decay_factor(seconds: u64, half_life_days: f64) -> f64 {
        if !(half_life_days > 0.0) {
            return 0.0;
        }
        let days = seconds as f64 / SECONDS_PER_DAY;
        2f64.powf(-days / half_life_days)
    }

    pub fn score_at(&self, now: u64, half_life_days: f64) -> f64 {
        let dt = now.saturating_sub(self.last_used);
        self.score * Self::decay_factor(dt, half_life_days)
    }

    pub fn record_use(&mut self, now: u64, half_life_days: f64) {
        let current = self.score_at(now, half_life_days);
        self.score = current + 1.0;       // “one more use”
        self.last_used = now;
        self.count = self.count.saturating_add(1);
    }
}
```

Then store uses it:

```rust
pub fn record_use(&mut self, path: &str) {
    let now = current_timestamp();
    let hl = self.half_life_days;

    self.entries
        .entry(path.to_string())
        .and_modify(|e| e.record_use(now, hl))
        .or_insert_with(|| FrecencyEntry {
            count: 1,
            last_used: now,
            score: 1.0,
        });

    self.dirty = true;
    // self.revision += 1;
}
```

And ranking becomes:

```rust
pub fn get_recent_items(&self, limit: usize) -> Vec<(String, f64)> {
    let now = current_timestamp();
    let hl = self.half_life_days;

    let mut items: Vec<_> = self.entries.iter()
        .map(|(path, entry)| (path.clone(), entry.score_at(now, hl)))
        .collect();

    items.sort_by(|a, b| {
        b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal)
            .then_with(|| a.0.cmp(&b.0))
    });

    items.truncate(limit);
    items
}
```

**Why this solves your concerns:**

* Rich-get-richer is greatly reduced: old usage decays away unless reinforced.
* Ordering changes are explainable: “each use adds +1; score halves every N days without use.”
* Retroactive half-life changes: yes, it will affect decay going forward (and thus ordering). If you want “apply only to future uses,” store `half_life_days_at_last_used` per entry and use that in `score_at`.

**Back-compat note:** your JSON already has a `score` field; with this new meaning, old files still load. If an old file has `score = 0.0` (because it was missing), you can initialize `score = count as f64` on load for a smoother migration.

---

### 5.5 Fix cache invalidation without re-filtering everything

Add a simple revision counter:

```rust
pub struct FrecencyStore {
    entries: HashMap<String, FrecencyEntry>,
    file_path: PathBuf,
    dirty: bool,
    half_life_days: f64,
    revision: u64,
}

impl FrecencyStore {
    pub fn revision(&self) -> u64 { self.revision }

    pub fn record_use(&mut self, path: &str) {
        // ... update entry ...
        self.dirty = true;
        self.revision = self.revision.wrapping_add(1);
    }

    pub fn set_half_life_days(&mut self, half_life_days: f64) {
        if (self.half_life_days - half_life_days).abs() > 0.001 {
            self.half_life_days = half_life_days;
            self.revision = self.revision.wrapping_add(1);
        }
    }
}
```

Then any cached “filtered list” can store `frecency_revision`. If it changes, you can:

* re-sort (cheap) if membership same
* or rebuild if necessary

This is the lowest-friction way to kill the known bug.

---

### 5.6 Make saves async + batched (debounce) and atomic

**Debounce**: don’t save on every `record_use`. Save:

* after X ms of inactivity, or
* when app is backgrounded, or
* on shutdown

**Atomic write**: write temp then rename.

You can do this without introducing async runtimes:

* Maintain a background thread that receives “snapshot to save” messages (or “please save now”).
* On send, you clone the entries (or you serialize on the main thread and send the string; depends on tradeoffs).
* Write to `frecency.json.tmp`, `fsync` if you care, then rename.

Also, stop cloning for synchronous saves if you keep them:

```rust
#[derive(Serialize)]
struct FrecencyDataRef<'a> {
    entries: &'a HashMap<String, FrecencyEntry>,
}

let data = FrecencyDataRef { entries: &self.entries };
serde_json::to_writer(&mut file, &data)?;
```

That avoids `self.entries.clone()`.

---

### 5.7 Consider pruning to keep the file bounded

Even with decay, old never-used-again scripts sit forever.

Add something like:

* remove entries with `score_at(now) < 0.01` AND `last_used > 180 days ago`
* or keep top K entries + recently used

This keeps load/save fast forever.

---

### 5.8 User transparency: expose *reasons*, not raw floats

I would not surface “score = 3.42819” to normal users.

What I *would* expose (optional “debug / explain ranking”):

* “Used 12 times”
* “Last used 2 days ago”
* “This score halves every 7 days without use”
* maybe a small relative bar

That addresses “hard to explain ordering changes” without dumping math on people.

---

## Direct answers to your five conceptual questions (based on your code + constraints)

1. **Is exponential decay right?**
   Yes, for ranking it’s the best default: smooth, stable, and easy to tune. But implement *true half-life* and use the incremental frecency update so frequency decays too. Linear/step functions are easier to explain, but they tend to produce annoying cliff effects (“yesterday it was top; today it vanished”).

2. **How to invalidate caches without re-filtering everything?**
   Use a `revision` counter. On change, re-sort cached lists. If you want incremental updates: have `record_use()` return `{path, new_score}` so caches can “bubble” one item within already-filtered results instead of recomputing membership.

3. **Should saves be async/batched?**
   Yes. Debounce + background write + atomic rename. This is one of the highest ROI changes for “instant launcher” feel.

4. **How to handle rich-get-richer?**
   Your current model *creates* rich-get-richer. Fix it by switching to `score = score * decay(Δt) + 1` and ranking by `score_at(now)`. Optionally also dampen per-use contribution slightly (e.g., add `+0.7` instead of `+1` for extremely high-frequency scripts) but usually the decay model alone is enough.

5. **Expose frecency scores for transparency?**
   Expose “why” (counts + last used + half-life setting), not raw floats. Provide raw floats in a debug screen for power users.

---

If you want, I can also propose a small migration strategy for `frecency.json` that:

* keeps backward compatibility,
* converts old entries cleanly,
* and prevents ranking shocks on upgrade.
