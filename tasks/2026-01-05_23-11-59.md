## 1) Critical issues

### A. **LoggingGuard lifetime looks easy to get wrong (and your Phase 1 snippet *does* get it wrong)**

In `36-startup-phase1-pre-gpui.md` you show:

```rust
logging::init();
```

…but `logging::init()` returns a `LoggingGuard` whose whole job is to keep the non-blocking writer alive. If you don’t store it, it gets dropped immediately and you’ll silently lose / truncate logs.

**Fix:** make it impossible to misuse.

```rust
fn main() -> anyhow::Result<()> {
    let _log_guard = logging::init()?; // must be bound
    // ...
    Ok(())
}
```

**Even better:** have `init()` return `()` and stash the guard in a `static OnceLock<LoggingGuard>` inside the logging module so callers can’t accidentally drop it.

---

### B. **`truncate_for_log` can panic on valid UTF‑8**

You do:

```rust
&s[..max_len]
```

That slices by bytes, not characters, and will panic if `max_len` lands mid‑codepoint. Clipboard/log payloads regularly contain emoji and non-ASCII.

**Fix: clamp to char boundary:**

```rust
pub fn truncate_for_log(s: &str, max_len: usize) -> String {
    if s.len() <= max_len {
        return s.to_owned();
    }
    let mut end = max_len.min(s.len());
    while end > 0 && !s.is_char_boundary(end) {
        end -= 1;
    }
    format!("{}...({})", &s[..end], s.len())
}
```

---

### C. **Legacy migration as documented is internally inconsistent (high risk of breaking backwards compatibility)**

Your structure says new scripts live at:

* `~/.scriptkit/kit/main/scripts`

But your migration pseudocode says:

* move `~/.kenv/scripts` → `~/.scriptkit/kit/main/scripts` ✅
* then “Create symlink for backwards compatibility: symlink(new_scriptkit, old_kenv)” ❌

Remove all migration pseudocode. We're doing a clean break!

---

### D. **Multi-instance race conditions around PID/orphan cleanup**

You write a PID, then “cleanup orphans”, before GPUI starts.

If two instances launch close together (double-click, login item + manual, crash restart), you can get:

* each overwriting PID file
* each deciding the other’s scripts are “orphans”
* killing the wrong processes

**Fix:** enforce single-instance with a real OS lock (not just a PID file). On Unix/macOS: open a lock file and `flock` it. If lock fails, exit early (or send “show” over a socket/stdin protocol).

---

### E. **`libc::signal` is a weak/unsafe signal setup (and you don’t handle common gotchas)**

`signal()` has portability/semantics issues; you also don’t address `SIGPIPE` (common with closed pipes, especially with stdin/stdout protocols). Also, the whole block is `unsafe` for something that has good safe crates.

**Fix:** use `signal_hook` (or similar) to set an atomic flag safely and consistently.

---

### F. **Your “start hidden / don’t focus” contract is muddied by a forced focus**

You set:

```rust
show: false,
focus: false,
```

…and then immediately call `win.focus(...)`.

Depending on GPUI/macOS behavior, that can steal focus or cause activation weirdness. At minimum, it’s confusing: “don’t focus” but “focus anyway”.

**Fix:** keep focus state internal (store the focus handle) and focus only on first show, or only update GPUI’s internal focus without activating macOS.

---

### G. **Swizzling must be strictly one-shot**

You swizzle after window creation and again on first show (per your notes). Re-swizzling can cause:

* double-injection
* selector confusion
* hard-to-debug crashes

**Fix:** guard with `OnceLock` inside `swizzle_gpui_blurred_view()` so it becomes idempotent.

---

## 2) Performance concerns

### A. **`bun eval` config load is your startup tax; you should cache**

You already know it’s ~100–300ms and the slowest step. If “total to hotkey response” matters, you want:

* **fast path**: read cached config output (JSON) immediately
* **slow path**: re-eval with bun in background only if config.ts changed (mtime/hash)

This can drop “time-to-hotkey” dramatically on warm starts.

A pragmatic pattern:

* On successful bun eval, write `config.cache.json` + a metadata file (hash/mtime + app version).
* On startup, if cache is valid, use it instantly.
* Spawn bun eval anyway to refresh cache and apply live if changed.

---

### B. **Polling loops (100–200ms) are a constant CPU wakeup tax**

You have several loops like:

* config watcher poll 200ms
* script watcher poll 200ms
* tray poll 100ms
* shutdown poll 100ms

Even if each loop is light, waking ~10 times/second forever is noticeable on laptops and can show up in “Energy Impact”.

**Fix:** make them event-driven wherever possible:

* `recv().await` for channels
* for shutdown: a signal-to-channel bridge (or `signal_hook` iterator in a dedicated thread sending into an async channel)
* tray: if the tray lib forces polling, keep it, but everything else should be event-driven + debounced.

---

### C. **Thread-per-scheduled-script wait does not scale**

For each scheduled run, you spawn:

* a bun process
* a waiting thread for that child

If schedules spike (or scripts run long), you can easily end up with dozens/hundreds of threads.

**Fix options (pick one):**

1. A single “reaper” thread that tracks children and `try_wait()`s them periodically.
2. On Unix: use `waitpid`/SIGCHLD handling (more complex, best scaling).
3. If you already have an async runtime: use async process waiting.

---

### D. **JSONL log file growth is unbounded**

Appending forever means:

* giant log files
* slow greps
* disk bloat

**Fix:** rolling logs (daily or size-based) with retention.

---

### E. **Script reload path can easily do heavy IO on the UI thread**

Your script watcher ultimately calls things like `view.refresh_scripts(ctx)` (which likely scans disk). You claim it’s ~5ms for ~331 scripts, but that’s a best-case. Users can have thousands, network home dirs, antivirus, etc.

**Fix:** do the scan off-thread and then apply results in `cx.update`.

---

## 3) API design feedback

### A. Centralize paths early (`SK_PATH` must affect *everything*)

Right now, logging init happens before `ensure_kit_setup()`, and your logging doc hardcodes `~/.scriptkit/logs`.

You want a single `paths` module used everywhere:

* `kit_root()` (respects `SK_PATH`)
* `logs_dir()`
* `db_dir()`
* etc.

Then logging uses `paths::logs_dir()` and you don’t get split-brain paths (especially important when `SK_PATH=/tmp/test-kit` for testing).

---

### B. Use typed enums instead of stringly categories

You currently have:

* compact mode category codes (single char)
* legacy `log(category: &str, message: &str)`

That’s brittle and easy to misuse.

Prefer:

```rust
#[derive(Copy, Clone)]
pub enum LogCat { App, Tray, Hotkey, Stdin, Perf, Error /* ... */ }

impl LogCat {
    pub fn code(self) -> char { /* ... */ }
    pub fn as_str(self) -> &'static str { /* ... */ }
}
```

Then `logging::log(LogCat::App, "…")` and both JSON + compact formats stay consistent.

---

### C. Standardize “optional subsystem” initialization

Your docs say “graceful degradation”, but some snippets still use `?` / `unwrap()` in places where you don’t want a crash (tray, MCP).

Give yourself one pattern:

```rust
fn init_optional<T>(name: &str, f: impl FnOnce() -> anyhow::Result<T>) -> Option<T> {
    match f() {
        Ok(v) => Some(v),
        Err(e) => { logging::warn(LogCat::App, format!("{name} disabled: {e}")); None }
    }
}
```

Then you’ll stop accidentally turning optional features into fatal failures.

---

### D. Unify your channel story

You have:

* `async_channel` for most things
* `std::sync::mpsc` for scheduler

The result is more threads and polling.

Either:

* switch scheduler to `async_channel`, or
* use `crossbeam_channel` (nice select support), or
* wrap the mpsc receiver in a thread that forwards into async_channel (simple bridge).

---

## 4) Simplification opportunities

### A. Replace `Arc<Mutex<Option<Entity<...>>>>` with `OnceLock`

That holder is a code smell and it’s easy to deadlock/panic if something goes wrong in the window constructor.

Use:

```rust
static APP_ENTITY: OnceLock<Entity<ScriptListApp>> = OnceLock::new();
```

or pass an `Arc<OnceLock<...>>` into the closure and `set()` it once.

---

### B. Make watchers truly event-driven + debounced (one pattern everywhere)

Right now, config/script watchers use polling with `try_recv()` while others are event-driven. You can use a single pattern:

* wait for first event with `recv().await`
* debounce window (e.g. 50ms)
* drain the rest with `try_recv()` in a tight loop
* apply one update

That’s simpler *and* faster and avoids idle CPU.

---

### C. Don’t “mem::forget” long-lived managers

That’s an intentional leak. It works, but it’s the kind of thing that bites you later (and it complicates leak detection).

Prefer “own it for the life of the thread”:

```rust
std::thread::spawn(|| {
    if let Ok(mut manager) = ExpandManager::new() {
        if manager.enable().is_ok() {
            loop { std::thread::park(); }
        }
    }
});
```

Or store it in a `OnceLock` global.

---

### D. Make “fallback entry point” event-driven

A fixed 500ms timer is a hack. Slow machines or delayed permissions can produce false negatives (showing window even though hotkey will succeed a moment later).

Better: hotkey init sends a “registered ok/failed” event; tray init returns a result; fallback triggers once both are known.

---

## 5) Specific recommendations with concrete code

### 5.1 Keep the logging guard alive (and make misuse impossible)

**Best fix:** stash guard internally.

```rust
// logging.rs
static GUARD: OnceLock<LoggingGuard> = OnceLock::new();

pub fn init_global() {
    let guard = init().unwrap_or_else(|e| {
        eprintln!("logging disabled: {e}");
        init_stderr_only()
    });
    let _ = GUARD.set(guard);
}
```

Then `main()` just calls `logging::init_global();`.

---

### 5.2 Rolling JSONL logs (stop unbounded growth)

```rust
use tracing_appender::rolling;

let file_appender = rolling::daily(&log_dir, "script-kit-gpui.jsonl");
let (non_blocking, guard) = tracing_appender::non_blocking(file_appender);
```

Add a simple retention cleanup (delete older than N days) at startup if you want.

---

### 5.3 Make compact timestamps unambiguous

Seconds-within-minute becomes ambiguous after 60s. Prefer a monotonic “t+ms” since process start.

Inside `CompactAiFormatter`:

* store a static `Instant` at first use
* print elapsed milliseconds

Example output: `t+012345|i|A|...`

---

### 5.4 Replace polling config watcher with event-driven + debounce

```rust
cx.spawn(|cx| async move {
    while let Ok(_ev) = config_rx.recv().await {
        // debounce window
        Timer::after(Duration::from_millis(50)).await;

        // drain additional events
        while config_rx.try_recv().is_ok() {}

        let _ = cx.update(|cx| {
            app_entity.update(cx, |view, ctx| view.update_config(ctx));
        });
    }
}).detach();
```

Do the same for script watcher, but coalesce paths + decide once whether you need a full reload.

---

### 5.5 Coalesce watcher sends to avoid backpressure/event storms

For “Changed” events, you don’t need 100 queued messages.

```rust
let (tx, rx) = async_channel::bounded(1);

// In watcher callback:
let _ = tx.try_send(ConfigEvent::Changed);
```

Now you get “at least one change happened” semantics with no backlog.

---

### 5.6 Scheduler: avoid one thread per child

A simple, robust “reaper thread” approach:

```rust
let running: Arc<Mutex<HashMap<u32, std::process::Child>>> = Arc::new(Mutex::new(HashMap::new()));

fn spawn_script(running: &Arc<Mutex<HashMap<u32, Child>>>, cmd: &mut Command) -> anyhow::Result<u32> {
    let child = cmd.spawn()?;
    let pid = child.id();
    running.lock().unwrap().insert(pid, child);
    Ok(pid)
}

std::thread::spawn({
    let running = running.clone();
    move || loop {
        {
            let mut map = running.lock().unwrap();
            map.retain(|pid, child| {
                match child.try_wait() {
                    Ok(Some(_status)) => {
                        PROCESS_MANAGER.unregister_process(*pid);
                        false
                    }
                    Ok(None) => true,
                    Err(e) => {
                        logging::log("SCHEDULER", &format!("wait error pid={pid}: {e}"));
                        PROCESS_MANAGER.unregister_process(*pid);
                        false
                    }
                }
            });
        }
        std::thread::sleep(std::time::Duration::from_millis(250));
    }
});
```

This caps waiting overhead to **one thread total**.

---

### 5.7 Signals: switch to a safe crate and stop polling shutdown

If you can bridge signals into an async channel, you can eliminate the 100ms shutdown poll. If you want to keep the atomic-flag approach, still use `signal_hook` to register cleanly.

---


### 5.9 Don’t focus hidden window at startup

If GPUI supports it, keep internal focus state but don’t activate macOS. If not, just delay the focus call until show:

```rust
fn show_main_window_helper(...) {
    // show window
    // then focus input
}
```

This aligns with launcher UX (no focus steal at login).

---

### 5.10 Make swizzling one-shot

```rust
static SWIZZLED: OnceLock<()> = OnceLock::new();

pub fn swizzle_gpui_blurred_view() {
    SWIZZLED.get_or_init(|| unsafe {
        // swizzle here
    });
}
```

---

## Bottom line

If you fix only a few things, fix these first:

1. **LoggingGuard lifetime** (it’s a silent-footgun)
2. **Remove migration pseudocode**
3. **Stop polling where you don’t need to** (config/script watchers + shutdown)
4. **Stop thread-per-child waits** (scheduler scaling)
5. **UTF‑8 safe truncation** (panic prevention)

If you want, I can also propose a cleaned-up “startup orchestrator” module layout that makes these phases explicit, enforces lifetimes, and standardizes “optional subsystem init” so the code matches your graceful-degradation intent.
