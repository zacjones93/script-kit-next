Below is a code-focused review of the three modules you shared, aimed at making search **correct**, **fast**, and **maintainable** under your constraints (instant launcher, 60fps scroll, multi-window, backwards compat).

---

## 1) Critical issues

### A. Your “half-life” math is wrong (and the docs/tests are reinforcing it)

In `src/frecency.rs` you label the parameter `half_life_days` and the comments say:

> After 7 days (half_life), the score is reduced to ~50%

But the implementation is:

```rust
let decay_factor = (-days_since_use / half_life_days).exp();
```

At `days_since_use == half_life_days`, this gives `e^-1 ≈ 0.368`, **not 0.5**. So you’ve implemented an **e‑folding time constant**, not a half-life.

This matters because:

* your config knob does not behave as advertised,
* any “tuning” you do will be misleading,
* users will never be able to reason about it.

**Fix**: use `ln(2)`:

```rust
let decay_factor = (-std::f64::consts::LN_2 * days_since_use / half_life_days).exp();
```

…and update tests accordingly.

---

### B. Configured half-life is silently ignored during `record_use()`

`FrecencyStore::with_config` sets `self.half_life_days`, and `load()` recalculates using that value.

But `record_use()` calls:

```rust
entry.record_use(); // inside FrecencyEntry
```

…and `FrecencyEntry::record_use()` calls:

```rust
self.recalculate_score(); // uses DEFAULT_SUGGESTED_HALF_LIFE_DAYS
```

So if a user configures half-life, their setting is **not applied** when they actually use scripts. Scores drift back to default until next reload / set_half_life call.

This is a real correctness bug.

---

### C. ASCII “case-insensitive” matching is being used on non-ASCII data, producing inconsistent ranking + highlighting

You currently use ASCII-only helpers (`to_ascii_lowercase`, byte compares) on data that may contain Unicode (script names, app names, window titles, descriptions, paths). This doesn’t “break” by crashing; it breaks by **quietly degrading** into case-sensitive matching for anything non-ASCII.

Consequence:

* `find_ignore_ascii_case`/`contains_ignore_ascii_case` may fail to detect exact matches for Unicode text.
* You still may match via `nucleo`, but you lose substring bonuses, and highlights can be wrong/empty.
* Ranking and UI highlighting can disagree (bad UX; hard to debug).

If you keep the ASCII fast path (reasonable), it must be **explicitly gated**.

---

### D. “Lazy indices” can become stale across query updates

`compute_match_indices_for_result(result, query)` computes highlight indices based on **the passed query**, but the result object may have been produced by a **previous query** (depending on how your UI task scheduling works).

That yields:

* highlights that don’t match the ranking query,
* flashing incorrect highlights during rapid typing,
* hard-to-reproduce UI glitches.

This needs a query generation guard (details below).

---

### E. `search_files()` path trimming is subtly wrong

In `src/file_search.rs`:

```rust
for line in stdout.lines().take(limit) {
    let line = line.trim();
```

macOS paths *can* contain leading/trailing spaces. Trimming changes the path. Rare, but when it happens it looks like “random failures”.

Use `trim_end_matches('\n')`-style behavior implicitly provided by `.lines()` already, and only skip empty lines without trimming.

---

## 2) Performance concerns

### A. `nucleo_score()` allocates every call (hot path)

Right now:

```rust
pub(crate) fn nucleo_score(...) -> Option<u32> {
    let mut haystack_buf = Vec::new();
    let haystack_utf32 = Utf32Str::new(haystack, &mut haystack_buf);
    pattern.score(haystack_utf32, matcher)
}
```

That `Vec::new()` is on the hottest loop in your app. In unified search you call nucleo scoring multiple times per item (name + filename + other fields) across potentially thousands of items. Even if allocations are small, allocator traffic will dominate.

**Fix**: reuse a buffer (and ideally wrap matcher+buffer into a small “context” object).

---

### B. Unified search rebuilds Pattern/Matcher multiple times per keystroke

In `fuzzy_search_unified_all`, you call:

* `fuzzy_search_builtins` (build Pattern + Matcher)
* `fuzzy_search_apps` (build Pattern + Matcher)
* `fuzzy_search_scripts` (build Pattern + Matcher)
* `fuzzy_search_scriptlets` (build Pattern + Matcher)
* etc.

That’s unnecessary repeated setup work on every query change.

**Fix**: build once per query (Pattern, Matcher, buffers, query_lower) and pass a shared context down.

---

### C. You recompute query-lower and display fields repeatedly

* `compute_match_indices_for_result` does `query.to_lowercase()` per visible row.
* Script filename and scriptlet display path are computed for every search call.

These are “death by a thousand cuts” issues.

**Fix**:

* precompute query_lower once per render frame / query generation and pass it,
* precompute stable display fields at load/index time.

---

### D. `mdfind` process spawn per keystroke will feel laggy unless debounced/cancelled

`Command::new("mdfind").output()` is synchronous and blocks until completion.

Even if you run it off the UI thread, doing it on every keystroke can flood the system and lead to stale results arriving out of order unless you implement:

* debounce,
* cancellation / generation IDs,
* result coalescing.

---

## 3) API design feedback

### A. Put “how to score” behind a `SearchContext` + `Scorer`

Right now scoring logic is copy/pasted across scripts/scriptlets/builtins/apps/windows with the same shape:

* substring bonuses
* nucleo fuzzy bonuses
* medium/low priority fields

This makes it hard to tune and easy to drift.

A cleaner API:

* `SearchContext { query_raw, query_ascii_lower (optional), pattern, matcher, utf32_buf }`
* `Scorer::score_fields(&mut ctx, &[FieldSpec]) -> i32`
* `FieldSpec { text: &str, weights: FieldWeights, kind: FieldKind }`

Then each search type becomes mostly a list of field specs.

---

### B. Don’t let highlighting use a different matching “truth” than ranking

Today ranking uses nucleo + substring bonuses, but highlighting uses a naive ASCII fuzzy indexer.

That guarantees disagreement in edge cases (repeats, Unicode, normalization).

Best pattern:

* choose *one* matching engine as canonical for “what matched where”
* use lazy computation and caching, but compute indices using the same logic as scoring (or at least a compatible fallback).

---

### C. Make scoring explainable by construction

Magic numbers scattered everywhere (`+100`, `+75`, `+50`, `/20`, etc.) makes tuning impossible.

Put weights into a struct that you can:

* tweak centrally,
* serialize to config,
* show in a debug “why did this rank” view.

---

## 4) Simplification opportunities

### A. Delete or quarantine `escape_query()`

`escape_query()` is unused, and the comment implies shell safety. But you’re not invoking a shell, so it’s irrelevant and potentially dangerous as “cargo cult” code.

Keep it only if:

* you truly plan to use a shell (I would not), or
* you want it for some other reason.

Otherwise remove it + its tests.

---

### B. Collapse duplicated search functions via a shared field scoring helper

The same scoring steps are repeated across item types. Extract:

* `score_substring_bonus(haystack, query_ascii_lower, start_bonus, mid_bonus)`
* `score_nucleo(haystack, ctx, base, scale_divisor)`
* `score_contains(haystack, query_ascii_lower, points)`

Then each search function becomes mostly a spec of fields and weights.

---

### C. Indices computation should not redo query lowercase

Make `compute_match_indices_for_result(result, query_lower: &str, mode: IndicesMode)`.

Or compute `QueryState { raw, lower, is_ascii }` once and reuse.

---

## 5) Specific recommendations with concrete changes

### Recommendation 1: Fix frecency half-life *and* apply configured half-life consistently

**Option A (recommended)**: make it a true half-life and keep the knob meaning “half-life”.

```rust
fn calculate_score(count: u32, last_used: u64, half_life_days: f64) -> f64 {
    let now = current_timestamp();
    let seconds_since_use = now.saturating_sub(last_used);
    let days_since_use = seconds_since_use as f64 / SECONDS_PER_DAY;

    // Guard against nonsense config
    let hl = half_life_days.max(0.001);

    // True half-life decay: 0.5^(days/hl) == exp(-ln2 * days/hl)
    let decay_factor = (-std::f64::consts::LN_2 * days_since_use / hl).exp();
    count as f64 * decay_factor
}
```

Then fix `record_use()` so it uses store config:

```rust
pub fn record_use(&mut self, path: &str) {
    let half_life = self.half_life_days;

    let entry = self.entries.entry(path.to_string()).or_insert_with(FrecencyEntry::new);
    entry.count += 1;
    entry.last_used = current_timestamp();
    entry.recalculate_score_with_half_life(half_life);

    self.dirty = true;
}
```

…and remove/stop using `FrecencyEntry::record_use()` if it can’t accept half-life.

This eliminates the silent config bug.

---

### Recommendation 2: Gate ASCII fast-path explicitly, and stop “silent Unicode breakage”

Add a small helper to choose the right path:

```rust
#[inline]
fn is_ascii_pair(a: &str, b: &str) -> bool {
    a.is_ascii() && b.is_ascii()
}
```

Then only award substring bonuses using ASCII match when both are ASCII:

```rust
if is_ascii_pair(&script.name, &query_lower) {
    if let Some(pos) = find_ignore_ascii_case(&script.name, &query_lower) {
        score += if pos == 0 { 100 } else { 75 };
    }
} else {
    // Optional: either skip substring bonus or do a slower Unicode-safe check.
    // I’d skip to keep perf predictable, relying on nucleo for Unicode.
}
```

Do the same for `contains_ignore_ascii_case` usage (especially on descriptions).

This avoids “Unicode names behave weird” reports.

---

### Recommendation 3: Stop allocating in `nucleo_score()` by reusing a buffer

Introduce a per-search context:

```rust
struct NucleoCtx {
    pattern: Pattern,
    matcher: Matcher,
    buf: Vec<u32>,
}

impl NucleoCtx {
    fn new(query: &str) -> Self {
        let pattern = Pattern::parse(
            query,
            nucleo_matcher::pattern::CaseMatching::Ignore,
            nucleo_matcher::pattern::Normalization::Smart,
        );
        Self {
            pattern,
            matcher: Matcher::new(nucleo_matcher::Config::DEFAULT),
            buf: Vec::new(),
        }
    }

    #[inline]
    fn score(&mut self, haystack: &str) -> Option<u32> {
        self.buf.clear();
        let utf32 = Utf32Str::new(haystack, &mut self.buf);
        self.pattern.score(utf32, &mut self.matcher)
    }
}
```

Then inside each search:

```rust
let mut nucleo = NucleoCtx::new(query); // not query_lower unless you truly want that

if let Some(s) = nucleo.score(&script.name) {
    score += 50 + (s / 20) as i32;
}
```

**This one change is likely your biggest win** for keeping search snappy.

---

### Recommendation 4: Build Pattern/Matcher once for unified search and pass it down

Change your unified search to create one context and share it:

```rust
pub fn fuzzy_search_unified_all(...) -> Vec<SearchResult> {
    let query_lower = if query.is_ascii() {
        query.to_ascii_lowercase()
    } else {
        query.to_lowercase()
    };

    let mut ctx = SearchCtx::new(query, &query_lower); // holds nucleo ctx etc.

    // pass &mut ctx into each sub-search
}
```

This eliminates repeated `Pattern::parse()` and matcher creation per category.

---

### Recommendation 5: Make lazy indices safe with a query generation ID + caching

Keep lazy indices (it’s a good approach for virtualization), but make it correct:

* When you produce results, attach a `query_id` (monotonic counter or hash of the query string).
* When computing indices, only compute if the requested `query_id` matches the result’s `query_id`.
* Cache indices by `(result_stable_id, query_id)` to avoid recomputation when scrolling.

Pseudo-approach:

```rust
struct SearchResult {
    query_id: u64,
    // ...
}

fn compute_indices(result: &SearchResult, query: &QueryState) -> MatchIndices {
    if result.query_id != query.id {
        return MatchIndices::default();
    }
    // compute...
}
```

This removes the “stale highlight during rapid typing” class of bugs without forcing you to compute indices upfront.

---

### Recommendation 6: Make highlighting match the same engine as ranking (or degrade explicitly)

If you can get indices from nucleo (ideal), use nucleo for highlighting too.

If you *can’t*, then at least:

* only use your ASCII indices function when both query and haystack are ASCII,
* otherwise return no indices (so the UI doesn’t lie).

That’s better than silently wrong highlights.

---

### Recommendation 7: Simplify and tame scoring with a normalized + explainable model

Right now score is a pile of constants. A more tunable, explainable approach is:

* **Primary**: text match score (nucleo + exact substring bonuses)
* **Secondary**: frecency as *bounded bonus* or tie-breaker

Bound frecency so it can’t bulldoze relevance:

```rust
// Example: cap frecency bonus
let frecency_bonus = (frecency_score * frecency_weight).min(MAX_FRECENCY_BONUS);
total = text_score + frecency_bonus;
```

Or lexicographic sorting:

* sort primarily by `text_score`
* break ties by `frecency_score`

This is much easier to explain to users (“we rank by match quality first; recent usage breaks ties”).

Expose to users as *one or two knobs*, not 20 constants:

* `recency_boost: 0..100`
* `half_life_days: number`
* maybe `prefer_exact_matches: bool`

Also: add a debug “ranking breakdown” panel that prints contributions (this will save you weeks of tuning guesswork).

---

### Recommendation 8: File search: `Command` is not shell-injectable, but process-spawn per keystroke is the real problem

On the security side: your current `Command::new("mdfind").arg(query)` is **not** shell injection because no shell is involved.

The bigger issues are:

* latency + CPU from spawning processes repeatedly,
* cancellation/race handling,
* parsing robustness (`trim()` issue).

Actionable changes:

1. **Remove `trim()`** on paths:

   ```rust
   for line in stdout.lines().take(limit) {
       if line.is_empty() { continue; }
       let path = Path::new(line);
       ...
   }
   ```

2. **Debounce** file search (e.g. 75–150ms) and use generation IDs so only newest results apply.

3. If you want a better long-term alternative: use Spotlight’s **Metadata framework** APIs (`MDQuery` / `NSMetadataQuery`) via FFI so you can:

   * stream results incrementally,
   * cancel cleanly,
   * avoid spawning processes,
   * avoid newline/path parsing issues.

That’s the “real” fix if file search is meant to feel as responsive as your script search.

---

## Direct answers to your 5 expert questions

1. **One backend vs keep ASCII fast path?**
   Keep ASCII as an *optimization*, but make nucleo the *semantic truth*. Gate ASCII paths with `is_ascii()` checks. Do not let ASCII logic affect Unicode results silently.

2. **Unicode safely without perf regression?**
   Use nucleo as canonical, and stop per-call allocations (buffer reuse). Only do ASCII substring bonuses when it’s actually ASCII.

3. **Lazy indices: right approach?**
   Yes for virtualization, but add `query_id` guards + caching. Otherwise you’ll keep chasing highlight flicker/staleness.

4. **Expose scoring weights to users?**
   Expose **high-level knobs** (recency boost + half-life + maybe exact-match bias), and provide an “explain ranking” breakdown view. Don’t expose every constant unless you also want to support it forever.

5. **Better than shelling out to `mdfind`?**
   Yes: Spotlight Metadata APIs (`MDQuery` / `NSMetadataQuery`). If you stick with `mdfind`, debounce + cancel + fix path parsing.

---

If you want, I can sketch a concrete `SearchCtx + FieldSpec` refactor that eliminates most duplicated scoring code in `search.rs` while keeping your current weights and backward behavior intact.
