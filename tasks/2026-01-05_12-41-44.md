## Vercel AI Gateway Anthropic-compatible API: the parts you can rely on

### Endpoint / base URL

Vercel’s Anthropic-compatible docs say the **base URL is `https://ai-gateway.vercel.sh`**, and the Messages endpoint is **`POST /v1/messages`** — so the full URL is:

`https://ai-gateway.vercel.sh/v1/messages`

That **does not match** the `https://api.vercel.com/v1/ai-gateway/anthropic/messages` URL in your notes.

### Auth header

Vercel supports **either** of these for an AI Gateway API key:

* `x-api-key: <AI_GATEWAY_API_KEY>`
* `Authorization: Bearer <AI_GATEWAY_API_KEY>`

OIDC tokens use `Authorization: Bearer <token>`.

### Streaming format

Their streaming event types include the **exact Anthropic-style events** (`message_start`, `content_block_start`, `content_block_delta`, …).
So your current streaming extraction logic (watching for `content_block_delta` + `text_delta`) is structurally compatible.

### Model IDs

Across AI Gateway docs/examples, model IDs are commonly **provider-prefixed** (e.g. `openai/gpt-5`, `xai/grok-4`, `anthropic/claude-sonnet-4.5`).
Given Vercel says you can route “any model available through the gateway” via this Anthropic-compatible API, you should assume **prefixed IDs are the “native” gateway way** and test whether raw Anthropic IDs are accepted as aliases.

### `anthropic-version` header

Vercel’s Anthropic-compat docs don’t call out `anthropic-version` as required. (They focus on base URL + auth.)
Practically: **keep sending it initially** (low-risk, drop-in with many Anthropic clients) and be ready to remove it if you see a 4xx that explicitly complains about headers.

---

# 1) Critical Issues (bugs / correctness / architectural footguns)

### 1. Anthropic provider ignores `ProviderConfig.base_url`

OpenAI honors `config.base_url` via `api_url()`. Anthropic hardcodes `ANTHROPIC_API_URL` in both `send_message` and `stream_message`. That makes your “minimal change approach” impossible without editing call sites.

**Fix:** implement `api_url()` for `AnthropicProvider` like you did for OpenAI, and use it everywhere.

---

### 2. Error handling drops the most useful information

`ureq` returns `Err(ureq::Error::Status(code, resp))` for non-2xx. Your `.context("Failed to send…")?` turns that into a generic error and you lose:

* status code
* provider error JSON/body

That will be brutal to debug (and makes UX bad: users see “failed” with no reason).

**Fix:** explicitly match `ureq::Error` and include the response body text (and ideally parse it into a nice message).

---

### 3. Anthropic non-stream parsing is wrong for multi-block text

Anthropic responses can include multiple content blocks. You only take `content[0].text`. You’ll silently truncate output.

**Fix:** concatenate all `content[*].text` blocks.

---

### 4. Streaming silently ignores tool calls / non-text deltas / error events

Your `AnthropicProvider::parse_sse_line` only emits text deltas and discards everything else. If the model uses tools, returns “thinking”, or the stream includes an error event, you’ll either:

* show an incomplete answer (no tool results)
* or fail with no error feedback

Even if your UI is “text-only”, you should at least detect tool-use and return a clear error like “tool calling not supported in this UI mode”.

---

### 5. Potentially wrong system message selection

You do:

```rust
let system_msg = messages.iter().find(|m| m.role == "system")
```

That picks the **first** system message, not necessarily the most recent or the intended “final system instruction”. If callers ever add a second system message later, it’s ignored.

**Fix:** either:

* take the last system message (`.rfind(...)`), or
* join all system messages with `\n\n`.

---

### 6. Blocking network calls will freeze the UI unless you’re careful

All provider methods are synchronous and do real HTTP I/O. If these are called on a GPUI/UI thread, you’ll get frame drops, input lag, and “app not responding” during long calls. That’s not a theoretical risk; it’s guaranteed.

**Fix:** run provider calls off the UI thread (worker thread) or move to async.

---

# 2) Performance Concerns (bottlenecks / inefficiencies)

### 1. No explicit timeouts

Without connect/read timeouts, a bad network can hang streaming forever. That looks like “stuck generation” and forces users to restart the app.

**Fix:** use a `ureq::Agent` with timeouts and keep it inside each provider.

---

### 2. Model list allocations on every call

`available_models()` returns a new `Vec<ModelInfo>` every time. Probably fine, but easy to optimize since model lists are essentially static.

**Fix:** store models as `&'static [ModelInfo]` (or lazy static) in `default_models`, or cache the vec in the provider.

---

### 3. SSE line parsing is allocation-heavy

`BufRead::lines()` allocates a `String` per line, then `serde_json::from_str()` allocates again. This is fine for typical token rates, but it’s not “cheap”.

**Fix (optional):** keep as-is for now; the bigger win is moving off UI thread + adding cancellation/timeouts.

---

# 3) API Design Feedback (patterns / abstractions)

### 1. Don’t use `String` for roles

Right now any string is accepted (e.g. `"usr"`). This pushes errors to runtime and to the provider.

**Better:** make it an enum:

```rust
enum Role { System, User, Assistant }
```

and only serialize to `"system" | "user" | "assistant"` at the edge.

---

### 2. Stream callback should be `FnMut(&str)` (or send events, not just text)

`Fn(String)` forces an allocation per chunk and makes it awkward to build accumulators without Mutex/Arc. Also, you’re baking in “text only” streaming.

**Better options:**

* `FnMut(&str)` for text-only streaming, or
* `FnMut(StreamEvent)` where StreamEvent can be TextDelta / ToolCall / Error / Done.

That will matter as soon as you add tool use or “thinking” display.

---

### 3. Provider config should carry headers/options cleanly

You already have `ProviderConfig` and `with_base_url`. Extend that pattern to cover:

* default headers (app attribution, custom routing headers, etc.)
* request timeouts
* max_tokens override

This avoids provider-specific one-offs.

---

# 4) Simplification Opportunities (remove duplication / unnecessary complexity)

### 1. Deduplicate SSE streaming loops

OpenAI and Anthropic both:

* send request
* `BufReader` lines loop
* skip blank lines
* parse “data: …”
* call callback

Extract a generic helper:

```rust
fn stream_sse_lines<R: BufRead>(reader: R, mut on_data: impl FnMut(&str) -> Result<()>) -> Result<()>
```

Each provider then only handles the JSON interpretation.

---

### 2. Stop using untyped `serde_json::Value` everywhere

It’s fine initially, but it’s easy to introduce subtle bugs (wrong key name, wrong type). At minimum, strongly type the response shapes you actually read.

This also makes it easier to support multi-block content and structured error responses.

---

# 5) Specific Recommendations (concrete code changes)

## A. Make AnthropicProvider support custom base URLs (required for Vercel)

**Minimal patch (mirrors OpenAIProvider):**

```rust
impl AnthropicProvider {
    pub fn with_base_url(api_key: impl Into<String>, base_url: impl Into<String>) -> Self {
        Self {
            config: ProviderConfig::new("anthropic", "Anthropic", api_key)
                .with_base_url(base_url),
        }
    }

    fn api_url(&self) -> &str {
        self.config.base_url.as_deref().unwrap_or(ANTHROPIC_API_URL)
    }
}
```

Then replace both `ureq::post(ANTHROPIC_API_URL)` call sites with:

```rust
ureq::post(self.api_url())
```

### Vercel constructor

Using the doc base URL + endpoint, the simplest working override is:

```rust
impl AnthropicProvider {
    pub fn with_vercel_gateway(api_key: impl Into<String>) -> Self {
        Self {
            config: ProviderConfig::new("anthropic", "Anthropic via Vercel", api_key)
                .with_base_url("https://ai-gateway.vercel.sh/v1/messages"),
        }
    }
}
```

**Important:** right now `ProviderRegistry` keys providers by `provider_id()`. If you want to keep both “Anthropic direct” and “Anthropic via Vercel” simultaneously, you must give Vercel a different `provider_id` (or you’ll overwrite the entry).

---

## B. Add proper error reporting (this is a quality-of-life must-have)

Create a helper:

```rust
fn send_json(request: ureq::Request, body: &serde_json::Value) -> Result<ureq::Response> {
    match request.send_json(body) {
        Ok(resp) => Ok(resp),
        Err(ureq::Error::Status(code, resp)) => {
            // Best effort body read
            let text = resp.into_string().unwrap_or_else(|_| "".into());
            anyhow::bail!("HTTP {}: {}", code, text);
        }
        Err(e) => Err(e).context("Network error")?,
    }
}
```

Then use it everywhere:

```rust
let response = send_json(
    ureq::post(self.api_url())
        .header("Content-Type", "application/json")
        .header("x-api-key", self.config.api_key())
        .header("anthropic-version", ANTHROPIC_VERSION),
    &body,
)?;
```

You can improve further by parsing known error formats and showing just the message.

---

## C. Fix Anthropic non-stream content extraction (stop truncating)

Replace:

```rust
let content = response_json
    .get("content")
    .and_then(|c| c.as_array())
    .and_then(|arr| arr.first())
    .and_then(|block| block.get("text"))
    .and_then(|t| t.as_str())
    .unwrap_or("")
    .to_string();
```

With something that joins all text blocks:

```rust
let content = response_json
    .get("content")
    .and_then(|c| c.as_array())
    .map(|arr| {
        arr.iter()
            .filter_map(|block| block.get("text").and_then(|t| t.as_str()))
            .collect::<String>()
    })
    .unwrap_or_default();
```

---

## D. Make streaming stop cleanly and surface errors

At minimum:

* break on `[DONE]` for OpenAI
* break on `type == "message_stop"` for Anthropic
* bail on `type == "error"`

That requires parsing more than just `content_block_delta`.

Example tweak (Anthropic):

```rust
fn parse_sse_line(line: &str) -> Option<serde_json::Value> {
    if !line.starts_with("data: ") { return None; }
    let json_str = &line[6..];
    if json_str == "[DONE]" { return None; }
    serde_json::from_str(json_str).ok()
}
```

Then in your loop:

```rust
if let Some(event) = Self::parse_sse_line(&line) {
    match event.get("type").and_then(|t| t.as_str()) {
        Some("content_block_delta") => { /* emit text_delta */ }
        Some("error") => {
            let msg = event.get("error")
                .and_then(|e| e.get("message"))
                .and_then(|m| m.as_str())
                .unwrap_or("Unknown streaming error");
            anyhow::bail!(msg);
        }
        Some("message_stop") => break,
        _ => {}
    }
}
```

That will make failures observable instead of “it just stopped”.

---

## E. Add cancellation (your UX will need this)

Streaming without cancel is a non-starter in a launcher/desktop app UX.

The simplest pattern:

* pass `Arc<AtomicBool>` cancel flag into `stream_message`
* check it inside the loop and return `Ok(())` when set

---

## F. Vercel-specific: support both auth styles (x-api-key vs Bearer)

Since Vercel supports both, you can keep `x-api-key` for minimal change.
But if you want maximum compatibility, add a small switch:

```rust
enum ApiKeyStyle { XApiKey, Bearer }

fn apply_auth(req: ureq::Request, key: &str, style: ApiKeyStyle) -> ureq::Request {
    match style {
        ApiKeyStyle::XApiKey => req.header("x-api-key", key),
        ApiKeyStyle::Bearer  => req.header("Authorization", &format!("Bearer {}", key)),
    }
}
```

Then Vercel can default to Bearer if you ever need it (OIDC parity, tooling consistency).

---

## G. Model list strategy for Vercel

Hard-coding “Claude models via Vercel” will get stale fast. The gateway has an OpenAI-compatible `/v1/models` endpoint (per Vercel docs) and examples show provider-prefixed model IDs.

Actionable approach:

* keep your current static lists for offline/instant UI
* add an optional “refresh models” in the AI window that fetches gateway models and caches them

That keeps launcher performance snappy while letting power users stay current.

---

## URLs

* [https://vercel.com/docs/ai-gateway/anthropic-compat](https://vercel.com/docs/ai-gateway/anthropic-compat)
* [https://vercel.com/docs/ai-gateway/openai-compat](https://vercel.com/docs/ai-gateway/openai-compat)
* [https://vercel.com/docs/ai-gateway/authentication](https://vercel.com/docs/ai-gateway/authentication)
* [https://ai-gateway.vercel.sh/v1/messages](https://ai-gateway.vercel.sh/v1/messages)
