## Critical issues

### 1) Your proposed Vercel base URL is wrong for “OpenAI-compatible”

You proposed:

```rust
const VERCEL_GATEWAY_URL: &str = "https://api.vercel.com/v1/ai-gateway";
```

If you’re implementing the **OpenAI-compatible API**, Vercel’s docs show the base URL as:

* `https://ai-gateway.vercel.sh/v1`

So your provider will 100% fail if you wire it to `api.vercel.com/v1/ai-gateway` and then try to call `/chat/completions`.

**Action:** treat Vercel Gateway as an OpenAI-compatible base URL and append `/chat/completions`, `/models`, etc.

---

### 2) Streaming parser is fragile (CRLF + no “done” break)

Both OpenAI + Anthropic streaming code does:

* `for line in reader.lines() { ... }`
* parse only if `line.starts_with("data: ")`
* ignores blank line separators (fine)
* **does not trim `\r`**
* **does not break on `[DONE]`** (it just returns `None` and keeps reading)

Two concrete failure modes:

* If the server uses `\r\n`, `line` may end with `\r`, your JSON parse can fail randomly.
* If the connection stays open (or proxies buffer weirdly), you can hang indefinitely after `[DONE]`.

Vercel’s OpenAI-compatible streaming explicitly follows the OpenAI SSE format and ends with `data: [DONE]`.

**Action:** trim CR and explicitly stop on `[DONE]`.

---

### 3) HTTP error handling is misleading and throws away useful details

`ureq` returns `Err(ureq::Error::Status(code, response))` for non-2xx. Your code does:

```rust
.send_json(&body)
.context("Failed to send request to OpenAI API")?;
```

That collapses *all* status errors into “failed to send request” and loses:

* the HTTP status code
* the provider’s JSON error message (`{"error": {...}}`)
* any retryability signal

Vercel’s gateway error format is OpenAI-style with standard HTTP status codes and an `error` object.
You should parse and surface this, not bury it.

---

### 4) **No timeouts** + synchronous API = easy UI freezes

Your provider methods are synchronous and there are **no explicit connect/read timeouts**. If these methods ever run on a UI thread (or on a thread that holds UI locks), you will freeze the app on:

* DNS stalls
* slow TLS negotiation
* hung connections
* slow streaming that never completes

Even if you currently call them from a background thread, no timeouts means “hang forever” is still on the table.

**Action:** add timeouts at the HTTP client layer and treat timeouts as retryable.

---

### 5) ProviderRegistry model lookup will bite you with Vercel

`find_provider_for_model()` scans providers and checks `available_models()` lists for an exact `m.id == model_id`.

Problems:

* It’s O(P * M) every time you call it.
* It assumes **model IDs are globally unique across all providers**.
* With Vercel, this becomes messy unless you namespace model IDs.

Vercel’s model IDs are explicitly provider-qualified like `openai/gpt-5`, `xai/grok-4`, etc.
If you *don’t* namespace (or don’t prevent collisions), whichever provider HashMap iteration hits first “wins” — nondeterministic behavior.

---

## Performance concerns

### 1) Streaming callback frequency can tank your UI

You call `on_chunk(text)` for every delta. OpenAI-style streaming often yields tiny chunks (sometimes single tokens). If `on_chunk` triggers a UI rerender per call, you’ll get jank in your chat window.

**Action:** buffer chunks and flush at ~16ms (or by character count), or push deltas into a channel and coalesce on the UI side.

---

### 2) Rebuilding model vectors repeatedly

Each `available_models()` returns a fresh `Vec<ModelInfo>`. If your UI polls models frequently (model picker, settings, etc.) you’ll allocate constantly.

**Action:** cache model lists per provider (and for Vercel, cache the `/models` response).

---

### 3) Registry model resolution is unnecessarily expensive

Repeated scans + repeated `available_models()` allocations compound. It’s not huge now, but once you add Vercel and more models, it gets dumb fast.

**Action:** build `HashMap<model_id, provider_id>` once at registration time, detect duplicates, and then lookup is O(1).

---

## API design feedback

### 1) Model identity should be **(provider_id, model_id)** not just `model_id`

Your registry currently behaves like model IDs are global. That’s barely true today and becomes false the moment:

* the same model is offered by multiple providers
* you add “gateway” providers
* you add any aliasing

**Best fix (clean):**

* represent a selection as `{ provider_id, model_id }`
* store that in your chat state
* stop doing global model resolution by string

**Back-compat strategy:**

* accept legacy raw IDs (`gpt-4o`) but normalize internally to a canonical form.

---

### 2) You need a shared “OpenAI-compatible provider” base

OpenAI + Vercel Gateway are the same protocol shape (chat completions + SSE). Right now you’ll copy/paste request building + SSE parsing + error handling.

**Action:** make a reusable helper/module that:

* builds OpenAI chat request bodies
* sends request with proper headers
* parses OpenAI SSE (robustly)
* parses OpenAI-style errors into a typed error

Then OpenAI provider and Vercel provider are basically config differences (base URL + auth key + model normalization).

---

### 3) Provider capability flags (you will want them)

Vercel supports gateway extensions like **fallback models** and **providerOptions.gateway.order** (routing order), etc.
Your current trait has no way to express “this provider supports gateway extensions” or “supports images/tool calls/reasoning”.

**Action:** add capabilities to `AiProvider` (even if it’s just a bitflag/struct):

* `supports_streaming`
* `supports_tools`
* `supports_images`
* `supports_reasoning_stream`
* `supports_fallback_models`
* etc.

This prevents “try feature, crash silently” UX.

---

## Simplification opportunities

### 1) Replace stringly-typed roles with an enum

`ProviderMessage.role: String` invites bugs (`"User"` vs `"user"`). Move to:

```rust
enum Role { System, User, Assistant }
```

…and serialize to strings at the boundary.

---

### 2) Don’t special-case system messages differently per provider in the call site

You already encapsulate Anthropic’s system handling in `build_request_body()`. Good. Keep that pattern: the rest of the app should pass a single `Vec<ProviderMessage>` and never care.

Just fix the “only first system message” issue (see below).

---

## Specific recommendations (concrete changes)

### A) Fix SSE parsing (CRLF + multi-line + done)

You want a stream loop that:

* trims `\r`
* accumulates `data:` lines until blank line
* breaks on `[DONE]`

Example approach:

```rust
fn stream_openai_sse<R: BufRead>(
    reader: R,
    mut on_delta: impl FnMut(&serde_json::Value) -> Result<()>,
) -> Result<()> {
    let mut data_buf = String::new();

    for line in reader.lines() {
        let mut line = line.context("Failed to read SSE line")?;
        if line.ends_with('\r') {
            line.pop();
        }

        // Blank line: end of event
        if line.is_empty() {
            if data_buf.is_empty() {
                continue;
            }
            if data_buf == "[DONE]" {
                break;
            }

            let v: serde_json::Value =
                serde_json::from_str(&data_buf).context("Invalid SSE JSON payload")?;
            on_delta(&v)?;
            data_buf.clear();
            continue;
        }

        // Collect data lines
        if let Some(d) = line.strip_prefix("data: ") {
            // SSE allows multiple data lines; JSON will usually be 1 line.
            if !data_buf.is_empty() {
                data_buf.push('\n');
            }
            data_buf.push_str(d);
        }
    }

    Ok(())
}
```

Then OpenAI provider becomes:

* parse JSON once per event
* extract `choices[0].delta.content` (and optionally reasoning fields)

And your Vercel provider can reuse the exact same code because Vercel OpenAI-compatible streaming follows the OpenAI SSE spec.

---

### B) Preserve full Anthropic text (not just first block)

Right now:

```rust
let content = response_json
  .get("content")
  .and_then(|c| c.as_array())
  .and_then(|arr| arr.first())
  .and_then(|block| block.get("text"))
  ...
```

If Anthropic returns multiple content blocks, you drop them.

Better:

```rust
let content = response_json
    .get("content")
    .and_then(|c| c.as_array())
    .map(|arr| {
        arr.iter()
            .filter_map(|b| b.get("text").and_then(|t| t.as_str()))
            .collect::<Vec<_>>()
            .join("")
    })
    .unwrap_or_default();
```

---

### C) Handle multiple system messages for Anthropic deterministically

You currently pick the first system message. If there are multiple, you probably want:

* concatenate in order with `\n\n`
* or take the last

I’d concatenate (predictable, preserves intent).

---

### D) Parse ureq status errors and surface provider error messages

Do not `.context("Failed to send request")?` on the raw send call.

Pattern:

```rust
fn send_or_parse_error(req: ureq::Request, body: &serde_json::Value) -> Result<ureq::Response> {
    match req.send_json(body) {
        Ok(res) => Ok(res),
        Err(ureq::Error::Status(code, res)) => {
            let text = res.into_body().read_to_string().unwrap_or_default();

            // Try OpenAI-style {"error": {"message": ...}}
            let msg = serde_json::from_str::<serde_json::Value>(&text)
                .ok()
                .and_then(|v| v.get("error")?.get("message")?.as_str())
                .unwrap_or(text.as_str())
                .to_string();

            anyhow::bail!("HTTP {}: {}", code, msg);
        }
        Err(e) => Err(anyhow::anyhow!(e)).context("Network error"),
    }
}
```

This is *especially* important for Vercel, because they document standard HTTP codes + OpenAI-style error bodies.

---

### E) Add explicit HTTP timeouts

At minimum:

* connect timeout (5–10s)
* read timeout (streaming: longer, but not infinite)

If you stay with `ureq`, use an `Agent` with timeouts and reuse it for keep-alive.

---

### F) Make ProviderRegistry O(1) for model → provider

When you register providers, build a model map:

```rust
pub struct ProviderRegistry {
    providers: HashMap<String, Arc<dyn AiProvider>>,
    model_to_provider: HashMap<String, String>,
}

pub fn register(&mut self, provider: Arc<dyn AiProvider>) {
    let pid = provider.provider_id().to_string();

    for m in provider.available_models() {
        if let Some(existing) = self.model_to_provider.insert(m.id.clone(), pid.clone()) {
            tracing::warn!(
                model_id = %m.id,
                provider_a = %existing,
                provider_b = %pid,
                "Duplicate model id registered; model resolution is now ambiguous"
            );
        }
    }

    self.providers.insert(pid, provider);
}

pub fn find_provider_for_model(&self, model_id: &str) -> Option<&Arc<dyn AiProvider>> {
    self.model_to_provider
        .get(model_id)
        .and_then(|pid| self.providers.get(pid))
}
```

This prevents the Vercel addition from becoming a nondeterministic mess.

---

## Vercel AI Gateway implementation answers (your key questions)

### 1) Default provider behavior

Make Vercel the default **when `SCRIPT_KIT_VERCEL_API_KEY` is set**, but:

* **do not override an explicit user choice** (store last-used provider in settings)
* keep direct providers available if keys are present (optional)

Reason: if someone sets the key, they likely want the gateway, and it’s the only way you can take advantage of gateway routing/observability automatically.

---

### 2) Model routing / IDs

Use Vercel’s **native gateway model IDs**: `provider/model`, like:

* `openai/gpt-4.1-mini`
* `anthropic/claude-sonnet-4.5`
* `xai/grok-4`

This gives you:

* zero collisions with existing Script Kit IDs (`gpt-4o` etc)
* matches Vercel’s `/models` output
* matches how Vercel expects you to specify models

**Back-compat trick (recommended):**

* Accept unprefixed IDs inside Vercel provider by mapping them:

  * `"gpt-4o"` → `"openai/gpt-4o"`
  * `"claude-3-5-sonnet-20241022"` → `"anthropic/claude-3-5-sonnet-20241022"` (if exists)
* If mapping fails, return a clear error that includes a hint: “Try selecting a gateway model like `anthropic/...`”.

This preserves Script Kit scripts that pass old IDs while still letting advanced users pick explicit gateway models.

---

### 3) Fallback behavior when gateway is unavailable

There are *two* different “fallback” problems:

#### A) Provider/model failover (gateway is up, a provider is down)

Let Vercel handle it. They explicitly support:

* provider ordering (`providerOptions.gateway.order`)
* model fallback lists (gateway extension: `models: [...]` or `providerOptions.gateway.models`)
* BYOK fallback to system credentials if your key fails

So Script Kit doesn’t need to invent its own failover logic here.

#### B) Gateway itself is down (can’t connect / 5xx at the edge)

Default: **fail with a clear message** and a “Try again” action.

Optional (power-user toggle): “Fallback to direct provider if keys exist.”
If you do this, make it explicit because it changes:

* privacy expectations
* observability/tracing
* billing path

---

### 4) Streaming format

If you use Vercel’s **OpenAI-compatible endpoint**, you should parse it like OpenAI SSE:

* lines start with `data:`
* content arrives in `choices[0].delta.content`
* ends with `data: [DONE]`

So: **reuse your OpenAI SSE parser**. No provider-specific detection needed.

Also note: Vercel can stream **reasoning fields** in `delta.reasoning` / `delta.reasoning_details` for supported models.
You can ignore those for now, but don’t break if they appear.

---

### 5) Error handling UX

Vercel documents:

* standard HTTP error codes (400/401/403/404/429/500)
* OpenAI-style `{"error": {"message", "type", "code", ...}}`

Practical surfacing rules:

* **401**: “Invalid Vercel AI Gateway key (or missing).”
* **429**: “Rate limit exceeded — retry later.”
* **404**: “Model not found — refresh model list / check spelling.”
* **500**: “Gateway/provider error — retry.”
* Network errors/timeouts: “Can’t reach gateway.”

Also: include a “copy details” payload for debugging (status code + error.message + model id). Don’t dump user prompts into logs by default.

---

## One more pragmatic Vercel note: fetch model list instead of hardcoding

Gateway supports listing models via `GET /models` (OpenAI-compatible).
If you hardcode a static list, you’ll constantly lag behind.

Best compromise:

* ship a small curated “starter list” (fast startup, no network needed)
* lazy-refresh `/models` in the background and cache results to disk

(Do **not** block launcher/UI on `/models`.)

---

```text
URLs (references)
- https://vercel.com/docs/ai-gateway/openai-compat
- https://vercel.com/docs/ai-gateway/models-and-providers
- https://vercel.com/docs/ai-gateway/provider-options
- https://vercel.com/docs/ai-gateway/byok
- https://vercel.com/docs/llms-full.txt
- https://vercel.com/changelog/openai-compatible-api-endpoints-now-supported-in-ai-gateway
```
