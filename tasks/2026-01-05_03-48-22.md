## Top priority fixes (if you do nothing else)

1. **Stop reading full clipboard payload every 500ms.** It’s both a performance killer and *functionally wrong* (you miss repeated copies of identical content).
2. **Stop sharing one `rusqlite::Connection` behind a global `Mutex`.** It serializes everything, negates WAL benefits, and can block UI reads.
3. **Don’t refresh the entire entry cache from SQLite on every insert.** Update the in-memory cache incrementally or debounce refresh.
4. **Get images out of TEXT/base64.** Use `BLOB` or a content-addressed file store; keep base64 only at the API boundary for backwards compatibility.

Everything else is secondary compared to those.

---

# 1. Critical Issues (bugs / correctness / architecture)

### 1) Clipboard polling is **incorrect** for “same content copied twice”

In `clipboard_monitor_loop` you detect changes via:

* text: `last != &text`
* image: `last_image_hash != hash`

If a user copies the **same text twice in a row** (very common), your loop won’t call `add_entry`, so you **won’t update timestamp**, and the item won’t bubble to the top. Same for images.

This is a correctness bug, not just performance.

**Fix:** Use an OS “clipboard change counter / event” (details below). Content equality is not the same thing as “clipboard updated”.

---

### 2) Polling reads the *entire* clipboard contents every 500ms

Every loop iteration calls:

* `clipboard.get_text()` → allocates the whole string
* `clipboard.get_image()` → typically allocates full RGBA buffer

If the clipboard contains a large payload (you mentioned up to 100MB), you can end up repeatedly allocating and dropping enormous buffers. That will hammer CPU, allocator, and memory bandwidth, and can spill into UI jank and system-wide slowdown.

Even if you later “dedup” in SQLite, you’ve already paid the cost.

**Fix:** Only fetch payload when you know the clipboard changed.

---

### 3) Global `Arc<Mutex<Connection>>` defeats WAL and amplifies contention

`DB_CONNECTION: OnceLock<Arc<Mutex<Connection>>>`

This means:

* Only one DB operation runs at a time (including reads).
* `busy_timeout` is largely irrelevant because you’re blocking on a Rust mutex before SQLite ever contends.
* A slow operation (hashing large content, large delete, metadata scan, etc.) blocks everything else.

**Fix:** Don’t share one connection; either:

* **One DB writer thread** + message passing (best), or
* **Per-thread connections** (good), plus WAL.

---

### 4) Hashing + metadata extraction happens while holding the DB lock

In `add_entry`:

```rust
let conn = conn.lock()?;
let content_hash = compute_content_hash(content);
```

That means hashing a giant string holds the DB mutex longer than needed, blocking monitor/prune/UI.

**Fix:** precompute outside any DB lock/transaction.

---

### 5) “Dedup” exists, but it’s fragile if you ever add real concurrency

You currently dedup via:

* compute hash
* `SELECT id WHERE content_type+hash`
* then `UPDATE timestamp` or `INSERT`

Because everything is behind one mutex, you won’t race.
But the moment you move to multiple connections (which you should), **two writers can race** and both insert duplicates unless you enforce uniqueness at the DB level.

**Fix:** add a **UNIQUE** constraint/index and use an UPSERT pattern.

---

### 6) `auto_vacuum=INCREMENTAL` likely doesn’t do what you think on existing DBs

SQLite’s `PRAGMA auto_vacuum` must be enabled *before* the database is created, otherwise you need a full `VACUUM` rebuild once to switch modes.

Right now you run:

```sql
PRAGMA auto_vacuum = INCREMENTAL;
```

…but you never verify `PRAGMA auto_vacuum;` nor run the one-time rebuild if needed.

**Fix:** check and migrate (details below).

---

# 2. Performance Concerns (bottlenecks / memory / inefficiencies)

### 1) `refresh_entry_cache()` on every insert/update is expensive

`add_entry()` calls `refresh_entry_cache()` every time, which does:

* SQLite query: `LIMIT 500`
* allocate + map all rows
* store into `ENTRY_CACHE`

If clipboard changes often (or scripts spam the clipboard), this becomes self-inflicted DB churn.

**Fix:** update the cache incrementally (push/update/move one entry), or debounce refresh (e.g., at most once per 250ms).

---

### 2) Base64-in-TEXT storage for images is costly

Even though PNG helps, base64 adds ~33% size overhead plus encoding/decoding costs.

More importantly, it makes SQLite writes heavier and increases WAL churn.

**Fix:** store PNG bytes as `BLOB` or store files on disk and keep paths + hash in SQLite.

---

### 3) `compute_image_hash` collision risk + wasted work

`compute_image_hash` hashes only first 1KB of pixels. Collision isn’t likely, but it’s *possible* and would cause you to miss new images.

Also: it doesn’t matter much because you already pay the full cost by calling `get_image()` each poll.

**Fix:** change detection should not require fetching the image at all.

---

### 4) Startup migration `populate_existing_metadata` can be brutal

It loads full `content` for every row needing metadata. If the DB contains large images/text, this can be a huge startup hit.

**Fix:** do it in batches and/or in a background migration job, or better: don’t require reading `content` to get metadata (store width/height at insert time).

---

### 5) Mutexes in render path can cause jank (depending on GPUI usage)

`get_cached_image()` locks a global `Mutex<LruCache<...>>`.

If the UI thread hits this frequently and background threads are inserting/evicting, you can get contention at 60fps.

This might be fine today (lock held briefly), but it’s a common source of “random scroll hitch” later.

**Fix (optional but nice):** use UI-thread-owned cache updates via channel, or use a lock-free “snapshot” pattern for entry metadata (e.g., swap `Arc<Vec<_>>`).

---

# 3. API Design Feedback (better patterns / abstractions)

### 1) Introduce a real “service” boundary

Right now DB, monitoring, cache, and clipboard ops are a tangle of globals. That makes correctness and performance tuning harder than it needs to be.

A better shape:

* `ClipboardMonitor` (platform-specific change detection)
* `ClipboardHistoryStore` (SQLite + blob/file storage)
* `ClipboardHistoryCache` (metadata + decoded images)
* `ClipboardHistoryService` orchestrates them; exposes:

  * `subscribe_updates()`
  * `get_entries_meta(limit, offset)` (from cache)
  * `get_entry_content(id)` (lazy)
  * `copy_to_clipboard(id)`

You can keep the current public API as thin wrappers over the service for backwards compatibility.

---

### 2) Make “insert” accept structured payload + precomputed metadata

`add_entry(content: &str, content_type: ContentType)` forces you to re-derive metadata from content.

For images, you already know width/height from `arboard::ImageData`. Pass it.

Example:

```rust
pub struct NewEntry {
    pub content_type: ContentType,
    pub timestamp_ms: i64,
    pub content_hash: [u8; 32],        // or String
    pub text: Option<String>,
    pub png_bytes: Option<Vec<u8>>,
    pub text_preview: Option<String>,
    pub image_width: Option<u32>,
    pub image_height: Option<u32>,
    pub byte_size: usize,
}
```

Now DB insert/update is purely I/O and doesn’t do CPU-heavy work while holding locks.

---

# 4. Simplification Opportunities (remove complexity without losing capability)

### 1) Collapse your 3 DB-touching threads into **one DB worker**

You mentioned Monitor / Prune / Prewarm all racing for the SQLite lock.

The clean simplification is:

* Monitor thread does OS clipboard detection + reads clipboard payload **only when changed**.
* It sends `NewEntry` messages over a bounded channel to…
* One DB worker thread that owns the SQLite writer connection.
* Pruning is just a periodic message handled by the same worker.
* Prewarming reads from the cache or requests content through the worker, but doesn’t touch SQLite directly.

This removes the need for `Arc<Mutex<Connection>>` entirely.

---

### 2) Stop re-querying SQLite just to update list ordering

Cache can be the source of truth for list views; DB is persistence.

When you insert/update an entry, you already know:

* id
* timestamp
* type
* preview/dimensions/size

So you can update `ENTRY_CACHE` directly and skip `refresh_entry_cache()` except at startup and after destructive ops (clear history / bulk prune).

---

# 5. Specific Recommendations (concrete code changes)

## A) Switch to event/counter-based change detection (macOS-first)

### ✅ Recommendation

**Yes**: you should switch away from payload polling.

For macOS, the simplest “almost event-based” solution is polling `NSPasteboard`’s `changeCount` (cheap integer). Only when it changes do you fetch text/image.

Pseudo-structure:

```rust
#[cfg(target_os = "macos")]
fn pasteboard_change_count() -> i64 {
    // Use objc/cocoa/objc2 to call NSPasteboard.generalPasteboard.changeCount
}

fn clipboard_monitor_loop(...) {
    let mut last_change_count: Option<i64> = None;

    loop {
        let cc = pasteboard_change_count();
        if last_change_count != Some(cc) {
            last_change_count = Some(cc);
            // NOW fetch clipboard payload once and store
            capture_and_store(&mut clipboard);
        }

        thread::sleep(Duration::from_millis(50)); // cheap now
    }
}
```

### Platform notes (practical):

* **macOS:** `NSPasteboard` changeCount polling is usually good enough and far cheaper than reading payloads.
* **Windows:** real event-based via `AddClipboardFormatListener` (needs a message loop / hidden window).
* **Linux:** X11 can use XFixes events; Wayland is messy (often ends up polling).

Given you’re macOS-first, implement macOS changeCount now, keep your current polling as fallback elsewhere.

---

## B) Stop sharing one SQLite connection behind a Mutex

### Option 1 (best): DB worker thread (single writer)

This is the most robust for performance-critical UI.

**Core idea:**

* DB worker owns `rusqlite::Connection` (no mutex).
* Other threads send `DbRequest` messages.

Example sketch:

```rust
enum DbRequest {
    AddOrTouch(NewEntry),
    Prune { now_ms: i64 },
    GetContent { id: String, reply: std::sync::mpsc::Sender<Option<EntryContent>> },
}

fn db_worker_loop(rx: Receiver<DbRequest>) -> Result<()> {
    let conn = open_and_init_connection()?;
    for msg in rx {
        match msg {
            DbRequest::AddOrTouch(entry) => add_or_touch_impl(&conn, entry)?,
            DbRequest::Prune { now_ms } => prune_impl(&conn, now_ms)?,
            DbRequest::GetContent { id, reply } => {
                let _ = reply.send(get_content_impl(&conn, &id)?);
            }
        }
    }
    Ok(())
}
```

This also lets you do batching (e.g., coalesce multiple inserts per 50ms) if needed.

### Option 2 (good): per-thread connections + WAL

If you don’t want the worker yet, do this:

* Remove `DB_CONNECTION: Arc<Mutex<Connection>>`.
* Store the DB path globally.
* Use `thread_local!` to open one connection per thread (monitor thread, prune thread, UI thread).

This immediately eliminates Rust-level contention and allows WAL to actually help.

---

## C) Move hashing/encoding/metadata outside the DB lock

Even if you keep your current structure, this is easy and worth it.

Change `add_entry` to:

```rust
pub fn add_entry(content: &str, content_type: ContentType) -> Result<String> {
    if content_type == ContentType::Text && is_text_over_limit(content) {
        anyhow::bail!("...");
    }

    let timestamp = chrono::Utc::now().timestamp_millis();

    // Do CPU work BEFORE lock:
    let content_hash = compute_content_hash(content);
    let (text_preview, image_width, image_height, byte_size) =
        extract_metadata(content, content_type.clone());

    let conn = get_connection()?;
    let conn = conn.lock().map_err(|e| anyhow::anyhow!("Lock error: {}", e))?;

    // ... DB work only here ...
}
```

Right now, large hashes happen inside the mutex. That’s avoidable.

---

## D) Enforce dedup at the DB level (needed if you add concurrency)

Add a unique index:

```sql
CREATE UNIQUE INDEX IF NOT EXISTS ux_history_dedup
ON history(content_type, content_hash);
```

Then change logic to be race-safe. Example pattern (no `RETURNING` required):

```rust
// Try insert first (fast path)
let inserted = conn.execute(
    "INSERT OR IGNORE INTO history (id, content, content_hash, content_type, timestamp, pinned, ocr_text, text_preview, image_width, image_height, byte_size)
     VALUES (?1, ?2, ?3, ?4, ?5, 0, NULL, ?6, ?7, ?8, ?9)",
    params![&id, content, &content_hash, content_type.as_str(), timestamp, text_preview, image_width, image_height, byte_size as i64],
)?;

if inserted == 0 {
    // Already exists; just touch timestamp
    conn.execute(
        "UPDATE history SET timestamp = ? WHERE content_type = ? AND content_hash = ?",
        params![timestamp, content_type.as_str(), &content_hash],
    )?;

    let existing_id: String = conn.query_row(
        "SELECT id FROM history WHERE content_type = ? AND content_hash = ?",
        params![content_type.as_str(), &content_hash],
        |row| row.get(0),
    )?;
    return Ok(existing_id);
}

Ok(id)
```

This survives concurrent writers cleanly.

---

## E) Fix image storage without breaking compatibility

### Keep compatibility at the edges

Your scripts likely expect:

* text: raw string
* image: `png:{base64...}` or legacy `rgba:...`

You can keep that output format while changing internal storage.

### Best practical approach: **file-based blob store**

* Store PNG bytes on disk as: `~/.scriptkit/clipboard/blobs/<hash>.png`
* Store in SQLite:

  * `id`
  * `content_hash`
  * `content_type`
  * `timestamp`
  * `image_width`, `image_height`, `byte_size`
  * `blob_path` (or just derive path from hash)

Then `get_entry_content(id)` can:

* Read file
* Return `png:{base64}` if a caller needs the legacy string

This keeps SQLite lean and makes pruning/vacuum trivial.

### Alternative: SQLite BLOB column

Still better than base64 TEXT, but you’ll fight DB file growth and WAL size more.

---

## F) Stop calling `refresh_entry_cache()` on every change

### Incremental cache update

Add a cache API like:

```rust
pub fn upsert_entry_meta(meta: ClipboardEntryMeta) {
    if let Ok(mut cache) = get_entry_cache().lock() {
        cache.retain(|e| e.id != meta.id);
        cache.push(meta);
        cache.sort_by(|a, b| {
            b.pinned.cmp(&a.pinned).then_with(|| b.timestamp.cmp(&a.timestamp))
        });
        cache.truncate(MAX_CACHED_ENTRIES);
    }
}
```

Then in `add_entry`, instead of requerying SQLite, call `upsert_entry_meta(...)` with values you already computed.

If you still want occasional full resync, run it:

* at startup
* after prune/clear
* or on a slow timer (e.g., every 5–30 seconds) as a safety net

---

## G) Oversized text handling: make it consistent at runtime

Right now:

* Monitor skips oversize text at capture time (good).
* `trim_oversize_text_entries()` runs only at init (not great if the limit changes).

If `set_max_text_content_len` can be changed live, then after changing it you should trigger trimming (or at least a background trim) immediately.

Example (call site, not inside config module):

```rust
set_max_text_content_len(new_len);
thread::spawn(|| {
    let _ = trim_oversize_text_entries();
});
```

Also consider whether “trim” should mean **delete** (current behavior) vs store a placeholder/meta-only entry.

---

## H) WAL + VACUUM strategy for write-heavy workload

### WAL: yes

WAL mode is the right default for your mix of frequent inserts + reads.

### Checkpointing: make it more predictable

Instead of “every 10 prune cycles”, consider:

* `PRAGMA wal_autocheckpoint = 1000;` (or tune)
* optionally `PRAGMA journal_size_limit = <bytes>;`

### Incremental vacuum: yes, but verify auto_vacuum is really enabled

At startup:

1. `PRAGMA auto_vacuum;` (0=NONE, 1=FULL, 2=INCREMENTAL)
2. If it’s not INCREMENTAL and you want it, schedule **one-time** `VACUUM` when idle.

Do not run full vacuum frequently; it blocks and can be slow on big DBs.

---

# Answers to your 5 “Expert Questions”

### 1) Switch to event-based clipboard monitoring?

**Yes.** Even a cheap “change counter” poll is dramatically better than polling payloads. It fixes both performance *and* the correctness bug where repeated identical copies don’t update timestamps.

* macOS: use pasteboard change count
* Windows: clipboard update listener
* Linux: varies; fallback to polling if needed

### 2) SQLite vs file-based store for images?

**SQLite for metadata + file store for image blobs** is the sweet spot for your workload.
Base64-in-TEXT is the worst of both worlds (bigger + slower).

### 3) Handle DB lock contention?

Don’t do `Arc<Mutex<Connection>>`.

Best: **single DB worker thread** (one writer connection).
Good: **per-thread connections** + WAL, plus DB-level uniqueness for dedup.

### 4) Hash-deduplicate clipboard entries? Strategy for text/images?

**Yes**, but:

* Hash should be computed **once per change event**, not every poll.
* Consider switching to a faster hash (BLAKE3) for very large entries.
* For images, hash **PNG bytes (or raw pixels)**, not the base64 string.

### 5) WAL mode + periodic VACUUM right for write-heavy?

**WAL: yes.**
**Incremental vacuum: yes**, but only if auto_vacuum is truly enabled (may need a one-time VACUUM migration). Add `wal_autocheckpoint` to prevent WAL growth.

---

If you want a “minimal-change” plan that moves the needle fast:

1. macOS changeCount gating (stop payload polling)
2. move hashing/metadata outside DB lock
3. stop `refresh_entry_cache()` per insert (incremental cache updates)
4. enforce UNIQUE dedup + insert-or-ignore pattern
5. move images to file store (keep base64 only at API boundary)
