Below is a “tell-it-like-it-is” review of the current interactive execution plumbing, with concrete fixes. I’m going to prioritize the things that can actually bite you in production: hangs, UI freezes, partial stderr, orphaned children, and avoidable throughput killers.

---

## 1) Critical issues (bugs / races / “this will hang eventually”)

### 1) **UI-thread blocking risk on `sync_channel::send`**

You store `self.response_sender = Some(response_tx)` and (presumably) the UI uses it to send user input back to the script. But `mpsc::sync_channel::Sender::send()` **can block** when the buffer is full.

That means: if the script stops reading stdin (or the writer thread blocks on a big write), the response queue fills, and then a user action (submit, cancel, etc.) can **freeze the UI** if it calls `send()` directly.

**Fix:** never allow UI code to call a potentially blocking send. Use `try_send`, or move sending off the UI thread, or separate “user input” from “auto host replies” so user actions never block.

Concrete options are in the recommendations section.

---

### 2) **Deadlock/hang scenario via writer thread blocking on `stdin.write_all`**

You correctly split read/write to avoid classic stdout/stderr deadlocks, but you still have a nasty “pipe backpressure” chain:

* Writer thread can block in `stdin.write_all(...)` if the script isn’t reading (or if you’re sending a huge payload like base64 screenshot).
* While writer is blocked, it stops draining `response_rx`.
* `response_rx` fills (bounded 100).
* Reader thread tries to `send()` auto-responses (clipboard list, window list, screenshot result, etc.) → blocks.
* Reader thread stops reading stdout.
* Script blocks writing stdout (pipe fills) while waiting for response → **everyone stuck**.

Backpressure is good; **uninterruptible backpressure is not**.

**Fix:** you need a way to (a) never block the UI, and (b) ensure the IO layer can still make forward progress or fail fast (timeout/kill) when stdin isn’t draining.

---

### 3) **“stdout EOF == script exit” is not reliable**

You treat `Ok(None)` from `stdout_reader` (stdout closed) as the end-of-script trigger, then you wait on the child.

But in the real world, especially with JS runtimes and user scripts:

* A child process can inherit bun’s stdout/stderr (e.g. `stdio: "inherit"`-style) and keep the pipe open even after bun exits.
* That means you may **never see EOF** on stdout and your reader thread can hang forever even though bun is gone.

This is subtle, but it’s a classic “wrapper around Node/Bun” failure mode.

**Fix:** make the **bun process exit** the authoritative end-of-session signal, not stdout EOF. You can still drain stdout briefly after exit, but don’t depend on it closing.

---

### 4) **Stderr capture race / partial stderr**

Right now you:

* spawn a stderr reader thread that writes into a buffer
* on script exit you call `stderr_buffer.get_contents()`

There is no guarantee the stderr reader thread has finished reading the last bytes before you snapshot the buffer. If bun exits and stderr flushes late, you can show partial stack traces.

**Fix:** `spawn_stderr_reader` should return a `JoinHandle` (or “done” signal). On exit, wait for it (with a small timeout if you’re worried about inherited FD edge cases).

---

### 5) **Multiple receivers on the prompt channel can drop messages unpredictably**

You do:

```rust
let (tx, rx) = async_channel::bounded(100);
let rx_for_listener = rx.clone();
self.prompt_receiver = Some(rx);
cx.spawn(async move { while let Ok(msg) = rx_for_listener.recv().await { ... }});
```

If *anything* else in your app still reads `self.prompt_receiver`, you now have **two competing consumers**. Messages will be split between them nondeterministically.

If you truly migrated to the event-driven listener, don’t keep an extra receiver around unless you are 100% sure it is never polled.

**Fix:** make there be exactly one consumer. Store only the sender, or wrap dispatching in one place.

---

### 6) **Logging is currently a performance + privacy landmine**

You log entire JSON payloads and `Debug`-print the whole `Message`:

* base64 screenshots
* clipboard contents
* HTML payloads for `Div/Form`
* potentially secrets in `Env` prompts

This will:

* tank performance (alloc + formatting + file IO)
* create massive logs
* leak sensitive user data into logs

**Fix:** aggressively truncate logged payloads + add a “sensitive” classification. In normal builds, log only message type + IDs + lengths.

---

## 2) Performance concerns (latency, throughput, memory)

### 1) Per-message syscalls and debug checks

In the writer thread you do:

* `fcntl(F_GETFD)` on every write
* `flush()` after every message
* log multiple lines per message

That’s fine while debugging a dead pipe, but not for 60fps “launcher feel”.

**Fix:** compile-gate the heavy checks (`cfg!(debug_assertions)` or feature flag) and remove `flush()` unless you wrap stdin in a buffered writer (pipes don’t need flush for “deliver now”).

---

### 2) Bounded queues + “big messages” = latency spikes

Your protocol sends potentially huge payloads over JSON lines (screenshots base64, possibly editor content). A single big send can block the writer thread long enough to cause head-of-line blocking for small responses behind it.

**Fix:** prioritize/segregate:

* small control responses (Submit, success/error, etc.)
* big payload responses (screenshot_result, big grids)

This is where either “two queues + priority” or a “writer actor that can reorder” is worth it.

---

### 3) Thread-per-session overhead

You’re spawning:

* reader thread
* writer thread
* stderr reader thread

Per run. That’s probably okay if runs are infrequent, but Script Kit usage patterns can be “lots of tiny scripts” in quick succession.

**Fix (optional):** move to a small IO thread pool or a single session runner thread that handles all pipes. Or move to async IO (Tokio) if you already ship it.

---

## 3) API design feedback (patterns that scale)

### 1) Separate “Script → UI prompts” from “Host services”

Right now the reader thread does:

* protocol parsing
* routing to direct host services (clipboard, window, file search, screenshot)
* routing to UI prompts
* error reporting

That makes it hard to reason about correctness and backpressure.

**Better shape:**

* `ScriptSessionRunner` (IO + protocol)
* `HostServices` (clipboard/window/file search/screenshot)
* `UiBridge` (PromptMessage dispatch)

Then your reader loop becomes something like:

```rust
match msg {
  m if host_services.can_handle(&m) => writer.enqueue(host_services.handle(m)),
  m => ui.send(prompt_from(m)),
}
```

The current “giant function with 20 early continues” will keep growing and become fragile.

---

### 2) Treat backpressure as a policy, not a side-effect

Right now, “bounded channel blocks” is acting as your flow-control policy.

That’s dangerous because it can block the wrong threads (UI thread, reader thread) at the wrong time.

**Better:** explicitly decide:

* which messages are lossy/coalescable (`SetInput`, `SetActions`, HUD updates)
* which must be delivered in order (`ShowArg`, `ShowForm`, `Submit` responses)
* what to do when overwhelmed (drop/coalesce, kill script, or show “script misbehaving” error)

---

## 4) Simplification opportunities (less code, fewer failure modes)

### 1) Pull message handlers into functions

This chunk is begging to be refactored:

* selected text
* clipboard history
* clipboard read/write
* window list/action
* file search
* get bounds
* get state/layout
* screenshot

Each can be:

```rust
fn try_handle_host_service(msg: &Message) -> Option<Message>
```

and the reader loop becomes dramatically smaller and less error-prone.

---

### 2) Collapse duplicated “exit/error reporting” logic

Both EOF and Err(e) paths do:

* get exit code
* read stderr buffer
* parse stack trace, generate suggestions
* send ScriptError
* send ScriptExit

Factor that into one `finish_session(exit_status, stderr_buffer)` function.

---

## 5) Specific recommendations (concrete changes + examples)

I’ll answer your “Expert Questions” directly here, and give practical implementation patterns.

---

# Q1) Is bounded channel the right pattern here? What capacity?

### Prompt channel (script → UI)

**Bounded is right**, but capacity should reflect semantics:

* For prompts that require user interaction (`ShowArg`, `ShowForm`, etc.): the script should *already* be waiting for input. You don’t need a giant buffer. If a script is sending multiple prompts without waiting, it’s either buggy or doing something streamy.
* For “state update” messages (`SetInput`, `SetActions`, maybe HUD): you want **coalescing**, not buffering.

**Recommendation:**

* Keep a bounded channel, but smaller (e.g. **32**).
* Add coalescing for “latest state wins” messages so they don’t fill the queue.

A simple approach: split into two pipes:

1. **Prompt queue** (ordered, bounded small)
2. **UI state watch** (latest value only)

Example (conceptual):

```rust
// Ordered prompts:
let (prompt_tx, prompt_rx) = async_channel::bounded::<PromptMessage>(32);

// Coalesced UI state:
let (state_tx, state_rx) = tokio::sync::watch::channel::<UiStatePatch>(UiStatePatch::default());
// If no tokio: implement your own `Arc<Mutex<UiStatePatch>> + notify`.
```

If you don’t want a second mechanism, you can still coalesce in the sender by using `try_send` for coalescable messages and dropping if full.

---

### Response channel (UI/host → script)

Bounded is tricky because you **must not block the UI**. Also, user-driven responses are naturally low-rate, so “unbounded but guarded” is often fine.

**Recommendation:**

* Split responses into two classes:

  1. **User responses** (Submit/Cancel) — never block UI; can be unbounded with a tiny guard.
  2. **Auto host replies** (clipboard list, screenshot data) — bounded is okay; if it blocks, you’re effectively backpressuring the script that asked for it.

This is where `crossbeam_channel` shines because it supports `select!` and you can prioritize user responses.

Example shape:

```rust
use crossbeam_channel::{bounded, unbounded, select};

let (user_tx, user_rx) = unbounded::<Message>();        // never block UI
let (auto_tx, auto_rx) = bounded::<Message>(64);        // backpressure script

// Writer thread:
loop {
  select! {
    recv(user_rx) -> msg => { write(msg?); }
    recv(auto_rx) -> msg => { write(msg?); }
  }
}
```

If you want to keep `async_channel` everywhere:

```rust
let (user_tx, user_rx) = async_channel::unbounded::<Message>();
let (auto_tx, auto_rx) = async_channel::bounded::<Message>(64);
```

and the writer thread can poll user first (or use `try_recv` then blocking recv).

---

# Q2) How do we guarantee stderr is fully captured before reporting script exit?

Right now you can read the buffer too early.

**Make `spawn_stderr_reader` return both:**

* the buffer handle
* the join handle (or a oneshot “done”)

Then, on exit:

1. wait for bun to exit
2. wait for stderr reader to finish (short timeout if you’re paranoid)
3. snapshot buffer
4. report

Example API change:

```rust
pub struct StderrCapture {
    pub buffer: Arc<RingBuffer>,
    pub join: std::thread::JoinHandle<()>,
}

pub fn spawn_stderr_reader(stderr: ChildStderr, script_path: String) -> StderrCapture { ... }
```

Then in your EOF/exit path:

```rust
if let Some(capture) = stderr_capture {
    // Best effort: join so buffer is complete.
    let _ = capture.join.join();
    let stderr_text = capture.buffer.get_contents();
}
```

**Important:** combine this with the “bun exit is authoritative” change, otherwise you can still hang if children keep stderr open.

If you can’t reliably join (because pipes might remain open due to child inheritance), do:

* join with timeout (requires more plumbing), or
* stop reading after bun exits + short drain window.

---

# Q3) Should we use process groups (`setpgid`) to ensure clean child process cleanup?

Yes. If you want “Cancel” to actually mean cancel, you need process-group control on Unix/macOS.

**Recommendation:**

* When spawning bun, put it into its own process group (PGID = PID).
* On cancel/timeout, send signals to the group:

  * SIGTERM
  * after a short grace, SIGKILL

Rust snippet (in your spawn function, not shown here):

```rust
use std::os::unix::process::CommandExt;

let mut cmd = std::process::Command::new("bun");
// ... args, env, stdio setup ...

unsafe {
    cmd.pre_exec(|| {
        // Create a new process group for bun (pgid = pid).
        if libc::setpgid(0, 0) != 0 {
            return Err(std::io::Error::last_os_error());
        }
        Ok(())
    });
}

let child = cmd.spawn()?;
let pid = child.id() as i32;
// PGID will equal PID because of setpgid(0,0)
```

On cancel:

```rust
fn kill_group(pgid: i32, sig: i32) {
    unsafe { libc::kill(-pgid, sig); } // negative pid == process group
}
```

(Or use `killpg` if you prefer.)

Also: store PGID, not just PID.

---

# Q4) What’s the recommended pattern for subprocess timeout in Rust?

Timeout isn’t one thing. You need at least two:

1. **Total runtime timeout** (rarely what you want for interactive scripts)
2. **No-activity / no-protocol timeout** (more useful)
3. **Waiting-for-user timeout** (UX-driven)

For a launcher, the best default is usually:

* no “hard kill at N seconds”
* but if there’s no script output / no progress for N seconds, prompt the user.

### Practical pattern with your current threads

Add a watchdog thread with:

* `last_activity: Arc<AtomicU64>` (monotonic timestamp)
* update it whenever:

  * you receive a protocol message from stdout
  * you successfully write a response to stdin
  * user interacts with the prompt UI

Watchdog loop:

* every 250ms, check `now - last_activity`
* if exceeds threshold:

  * send a `PromptMessage::TimeoutWarning { ... }` to UI
  * if user chooses “Stop”, kill process group
  * if user chooses “Keep waiting”, reset activity or extend deadline

This avoids silently killing scripts and matches “interactive tool” UX.

### If you can adopt async (Tokio)

Tokio makes “wait + read + timeout + cancel” much cleaner with `select!`, but it’s a bigger refactor.

---

# Q5) Should script execution be cancellable mid-prompt, or only between prompts?

**Mid-prompt. Always.**

From a UX standpoint, “Cancel does nothing until the script decides it’s safe” is a bad experience, and it’s also how you end up with stuck sessions.

**Implementation suggestion: two-phase cancel**

1. Send a protocol-level “cancel” message (if you can add it without breaking scripts).
2. After a short grace (e.g. 200–500ms), SIGTERM the process group.
3. After another grace, SIGKILL.

This gives well-behaved scripts a chance to clean up, but guarantees the app can recover.

Also: on cancel, **close the channels** (drop senders) so:

* writer thread wakes up
* prompt listener exits
* you don’t accumulate zombies waiting on recv()

---

## Selected Text Injection (your concern #5)

You’re right: “capture selection, inject into global scope” is race-prone on rapid switching because it relies on external focus timing.

**Make selection a per-session snapshot, not global mutable state.**

Good options that preserve compatibility:

### Option A: capture once at spawn and pass via env var

* At the moment you start execution, capture selected text
* pass `KIT_SELECTED_TEXT` (or similar) env var to bun
* your preload (`sdk.ts`) reads it and sets the global

This makes it session-scoped and avoids cross-run contamination.

### Option B: preload blocks script evaluation until it fetches selection via IPC

If older scripts need `selectedText` immediately at top-level, you need it *before* the script runs. A preload can do:

* send `GetSelectedText` immediately
* await response
* set global
* then import/execute user script

That eliminates race from the host side, but adds a “start handshake” roundtrip. Usually fine.

---

## One more “high value” change: make bun exit authoritative

This is worth repeating because it prevents real hangs.

**Change the session lifecycle:**

* Spawn a “reaper” that waits for `child.wait()`
* When bun exits, you:

  * mark session finished
  * stop accepting new UI submits
  * drain stdout/stderr briefly (best effort)
  * then emit ScriptExit + errors

Don’t rely on stdout closing to decide exit.

Even if you keep threads, this is doable: the reaper thread can send an “exit status” message into the same prompt channel (or a dedicated internal channel) that the UI/runner listens to.

---

## Quick “do this next” checklist (highest ROI)

If you do nothing else, do these:

1. **Guarantee UI never blocks** on response sending (split user vs auto, or use `try_send` + background send).
2. **Kill process group**, not just PID (setpgid + kill(-pgid, SIGTERM/SIGKILL)).
3. **Make bun exit authoritative**, not stdout EOF.
4. **Join stderr reader** (or otherwise wait for it) before you snapshot stderr.
5. **Stop logging full payloads**; log type/ids/lengths only by default.

These five fixes eliminate the most common “my launcher froze / scripts hang / cancel doesn’t work / error output missing” complaints.

---
