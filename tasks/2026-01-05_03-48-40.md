Below is a blunt, “fix-it” review of the current watcher code, with concrete patterns you can drop in.

---

## 1) Critical issues

### A. `Drop` + `.join()` will hang your app

All four watchers call `join()` in `Drop`, but **none of the watcher loops have any way to exit** unless their internal channel is disconnected.

* `watch_loop` blocks forever on `watch_rx.recv()`.
* The sender (`watch_tx`) is owned by the notify callback, which is owned by the notify watcher, which is owned by the same thread that’s blocked on `recv()`.
* So the sender never drops → receiver never disconnects → thread never exits → `Drop::join()` blocks forever.

If these watcher structs are ever dropped (shutdown, window teardown, profile switch, tests, “reload subsystem”, etc.), you risk a hard hang.

**Fix:** add an explicit stop signal that the loop checks, and only join after you’ve signaled stop.

---

### B. Config/Theme debounce is *leading-edge*, not trailing-edge

Current behavior:

* First relevant event starts a 500ms timer.
* Additional events inside 500ms are ignored.
* After 500ms you reload.

That is **not** “reload 500ms after the last change”. It’s “reload 500ms after the first change”.

This can reload while the editor is still doing atomic saves / multi-phase writes. That’s where you get intermittent “half-written” reads or parse errors.

---

### C. Config/Theme watcher misses common save patterns (atomic rename / remove)

You only treat `Create` and `Modify` as relevant.

Many editors save by:

1. write temp file
2. rename temp → target (or remove target then rename)

`notify` may emit `Remove` and/or `Modify(Name(...))` / rename-ish events for this. Your code ignores those, so **config/theme reload can silently not fire** depending on editor and filesystem.

---

### D. ScriptWatcher debouncer has a real race (flush thread vs insert)

You have two threads touching the same `pending_events`:

* main watch loop inserts `(event, Instant::now())`
* flush thread periodically scans and removes expired entries

There’s a race where the flush thread can:

* acquire the lock,
* decide an entry is expired,
* remove it,
* **send it**,
  while the main thread is blocked waiting to update the timestamp for a newer event that already arrived.

Net effect: you can emit a reload **too early**, violating the “wait for quiet” guarantee.

Also: the flush thread is never stopped or joined → it can live forever.

---

### E. Silent death / no restart path

If the setup phase fails (create watcher / add watch), the thread exits and you only log a warning. The watcher object can’t restart because `tx` was `take()`n and is gone.

That’s survivable for “dev tool” usage, but for a launcher you want something more deterministic:

* either supervisor-restart,
* or propagate a “watcher unhealthy” state up so you can retry.

---

## 2) Performance concerns

### A. Unbounded channels can accumulate event storms

`std::sync::mpsc::channel()` is unbounded. On a big recursive scripts directory, a bulk change (git checkout, branch switch, npm install, unzip, etc.) can enqueue a ton of events and spike memory.

At minimum, you want **coalescing** rules (e.g., “if more than N events pending, collapse to `FullReload`”).

If you want backpressure, use a bounded channel (`crossbeam_channel::bounded` or `async_channel::bounded`) and drop/coalesce when full.

---

### B. Per-change `thread::spawn()` for config/theme

You spawn a new thread on the first event of each debounce window. Not catastrophic, but it’s needless churn and makes correctness harder.

Use a single worker loop with `recv_timeout` (examples below).

---

### C. Appearance watcher spawns a process every 2 seconds

`defaults read` every 2 seconds is relatively expensive (process spawn + I/O). It’s probably “fine” on a dev machine, but it’s battery/CPU noise in a launcher.

If you keep polling, consider:

* making the interval larger (e.g., 5–10s),
* or using a macOS-native notification approach later.

Not your biggest issue, but it’s measurable overhead.

---

## 3) API design feedback

### A. You’re repeating the same watcher pattern 3 times

ConfigWatcher and ThemeWatcher are nearly identical. ScriptWatcher is “same idea but bigger”.

You’ll get better correctness by making **one generic file-watching service** with:

* one notify pipeline,
* one debouncing strategy,
* shared shutdown + restart behavior,
* per-target routing.

Even if you keep separate watchers for isolation, the code should still share a common “watch thread skeleton” and “debounce engine”.

---

### B. Receiver type mismatch makes composition harder

* Config/Theme use `std::sync::mpsc::Receiver`
* Appearance uses `async_channel::Receiver`

Pick one. For GPUI, a bounded async channel is usually nicer because you can integrate with task executors and you can control memory.

---

## 4) Simplification opportunities

### A. Eliminate the flush thread entirely

Do debouncing inside the same loop that receives notify events using `recv_timeout` with a computed timeout.

This removes:

* the race,
* the extra thread,
* the constant 100ms wakeups,
* a bunch of mutex contention.

---

### B. Treat config/theme as “single logical reload”

You don’t need per-event kinds; you need “reload after quiet”. That’s a classic trailing debounce.

---

## 5) Specific recommendations with concrete code

Below are patterns that directly address your numbered concerns.

---

# 5.1 Correct debounce pattern (no races, trailing-edge, no extra threads)

This is the simplest correct pattern in pure std:

* One channel carries `notify` events.
* The same loop also implements the debounce timer using `recv_timeout`.
* Each new relevant event pushes the deadline out (trailing-edge).

### Debounced single-file watcher skeleton (Config/Theme)

```rust
use notify::{recommended_watcher, RecursiveMode, Watcher};
use std::ffi::OsString;
use std::path::{Path, PathBuf};
use std::sync::mpsc::{channel, Receiver, RecvTimeoutError, Sender};
use std::thread;
use std::time::{Duration, Instant};
use tracing::{info, warn};

enum Msg {
    Notify(notify::Result<notify::Event>),
    Stop,
}

/// Generic “watch one file, emit Reload after quiet period”.
fn watch_single_file_debounced<E: Clone + Send + 'static>(
    target_path: PathBuf,
    debounce: Duration,
    out_tx: Sender<E>,
    reload_event: E,
    stop_rx: Receiver<Msg>,
    msg_tx_for_callback: Sender<Msg>,
) -> notify::Result<()> {
    let target_name: OsString = target_path
        .file_name()
        .unwrap_or_else(|| std::ffi::OsStr::new(""))
        .to_owned();

    let watch_dir: &Path = target_path
        .parent()
        .unwrap_or_else(|| Path::new("."));

    let mut watcher = recommended_watcher(move |res| {
        // Keep callback tiny and non-blocking.
        let _ = msg_tx_for_callback.send(Msg::Notify(res));
    })?;

    watcher.watch(watch_dir, RecursiveMode::NonRecursive)?;

    info!(path = %watch_dir.display(), file = ?target_name, "Watcher started");

    let mut deadline: Option<Instant> = None;

    loop {
        let msg = match deadline {
            None => {
                // No pending reload => wait indefinitely for next msg
                match stop_rx.recv() {
                    Ok(m) => m,
                    Err(_) => break,
                }
            }
            Some(dl) => {
                // Pending reload => wait until deadline, unless a new event comes in
                let timeout = dl.saturating_duration_since(Instant::now());
                match stop_rx.recv_timeout(timeout) {
                    Ok(m) => m,
                    Err(RecvTimeoutError::Timeout) => {
                        // Quiet period ended => emit reload
                        let _ = out_tx.send(reload_event.clone());
                        deadline = None;
                        continue;
                    }
                    Err(RecvTimeoutError::Disconnected) => break,
                }
            }
        };

        match msg {
            Msg::Stop => break,

            Msg::Notify(Err(e)) => {
                warn!(error = %e, "notify delivered error");
            }

            Msg::Notify(Ok(event)) => {
                // Filter: does this event mention the target filename?
                let touches_target = event.paths.iter().any(|p| {
                    p.file_name()
                        .map(|n| n == target_name.as_os_str())
                        .unwrap_or(false)
                });

                // Treat *everything except Access* as potentially relevant.
                // This covers atomic saves (remove/rename) too.
                let relevant_kind = !matches!(event.kind, notify::EventKind::Access(_));

                if touches_target && relevant_kind {
                    // Trailing-edge debounce: reset deadline on every hit
                    deadline = Some(Instant::now() + debounce);
                }
            }
        }
    }

    Ok(())
}
```

### How to wire stop + join correctly in your watcher struct

Key: **the thread must have an explicit Stop message**, and Drop should send it before joining.

```rust
pub struct ConfigWatcher {
    tx: Option<Sender<ConfigReloadEvent>>,
    control_tx: Option<Sender<Msg>>,
    watcher_thread: Option<thread::JoinHandle<()>>,
}

impl ConfigWatcher {
    pub fn new() -> (Self, Receiver<ConfigReloadEvent>) {
        let (tx, rx) = channel();
        (
            Self {
                tx: Some(tx),
                control_tx: None,
                watcher_thread: None,
            },
            rx,
        )
    }

    pub fn start(&mut self) -> notify::Result<()> {
        let out_tx = self
            .tx
            .take()
            .ok_or_else(|| std::io::Error::other("watcher already started"))?;

        let (control_tx, control_rx) = channel::<Msg>();
        self.control_tx = Some(control_tx.clone());

        let target_path =
            PathBuf::from(shellexpand::tilde("~/.scriptkit/kit/config.ts").as_ref());

        let handle = thread::spawn(move || {
            let res = watch_single_file_debounced(
                target_path,
                Duration::from_millis(500),
                out_tx,
                ConfigReloadEvent::Reload,
                control_rx,
                control_tx,
            );

            if let Err(e) = res {
                warn!(error = %e, watcher = "config", "Config watcher failed");
            }
        });

        self.watcher_thread = Some(handle);
        Ok(())
    }
}

impl Drop for ConfigWatcher {
    fn drop(&mut self) {
        if let Some(tx) = self.control_tx.take() {
            let _ = tx.send(Msg::Stop);
        }
        if let Some(h) = self.watcher_thread.take() {
            let _ = h.join();
        }
    }
}
```

You can reuse the same generic function for ThemeWatcher by swapping the target path and event type.

**This single change fixes:**

* your current debounce race window,
* the “leading-edge” behavior,
* missing rename/remove events,
* and the shutdown/join hang.

---

# 5.2 Fix ScriptWatcher debounce + lifecycle (remove flush thread)

Do it in one loop with a dynamic timeout driven by the *next* pending expiration.

Also add a “storm collapse” rule: if pending grows beyond a threshold, send `FullReload` and clear.

```rust
use std::collections::HashMap;
use std::sync::mpsc::{channel, Receiver, RecvTimeoutError, Sender};
use std::time::{Duration, Instant};

enum ScriptMsg {
    Notify(notify::Result<notify::Event>),
    Stop,
}

fn next_deadline(pending: &HashMap<PathBuf, (ScriptReloadEvent, Instant)>, debounce: Duration) -> Option<Instant> {
    pending
        .values()
        .map(|(_, t)| *t + debounce)
        .min()
}

fn flush_expired(
    pending: &mut HashMap<PathBuf, (ScriptReloadEvent, Instant)>,
    debounce: Duration,
    out_tx: &Sender<ScriptReloadEvent>,
) {
    let now = Instant::now();
    let mut to_send: Vec<ScriptReloadEvent> = Vec::new();

    pending.retain(|_, (ev, t)| {
        if now.duration_since(*t) >= debounce {
            to_send.push(ev.clone());
            false
        } else {
            true
        }
    });

    for ev in to_send {
        let _ = out_tx.send(ev);
    }
}

fn watch_scripts_debounced(
    scripts_path: PathBuf,
    extensions_path: PathBuf,
    debounce: Duration,
    out_tx: Sender<ScriptReloadEvent>,
    msg_rx: Receiver<ScriptMsg>,
    msg_tx_for_callback: Sender<ScriptMsg>,
) -> notify::Result<()> {
    let mut watcher = recommended_watcher(move |res| {
        let _ = msg_tx_for_callback.send(ScriptMsg::Notify(res));
    })?;

    watcher.watch(&scripts_path, RecursiveMode::Recursive)?;
    if extensions_path.exists() {
        watcher.watch(&extensions_path, RecursiveMode::Recursive)?;
    }

    let mut pending: HashMap<PathBuf, (ScriptReloadEvent, Instant)> = HashMap::new();

    // Tune this: how many unique paths before you just FullReload
    const STORM_THRESHOLD: usize = 200;

    loop {
        let deadline = next_deadline(&pending, debounce);

        let msg = match deadline {
            None => match msg_rx.recv() {
                Ok(m) => m,
                Err(_) => break,
            },
            Some(dl) => {
                let timeout = dl.saturating_duration_since(Instant::now());
                match msg_rx.recv_timeout(timeout) {
                    Ok(m) => m,
                    Err(RecvTimeoutError::Timeout) => {
                        flush_expired(&mut pending, debounce, &out_tx);
                        continue;
                    }
                    Err(RecvTimeoutError::Disconnected) => break,
                }
            }
        };

        match msg {
            ScriptMsg::Stop => break,

            ScriptMsg::Notify(Err(e)) => {
                warn!(error = %e, watcher = "scripts", "notify delivered error");
            }

            ScriptMsg::Notify(Ok(event)) => {
                // If we see “weird” events (rename, etc.), you may want to FullReload
                // rather than trying to perfectly interpret.
                let kind = event.kind.clone();

                for path in event.paths.iter() {
                    if !is_relevant_script_file(path) {
                        continue;
                    }

                    let reload_event = match kind {
                        notify::EventKind::Create(_) => ScriptReloadEvent::FileCreated(path.clone()),
                        notify::EventKind::Modify(_) => ScriptReloadEvent::FileChanged(path.clone()),
                        notify::EventKind::Remove(_) => ScriptReloadEvent::FileDeleted(path.clone()),
                        // If you want to be safer:
                        // _ => ScriptReloadEvent::FullReload,
                        _ => continue,
                    };

                    pending.insert(path.clone(), (reload_event, Instant::now()));

                    if pending.len() >= STORM_THRESHOLD {
                        pending.clear();
                        let _ = out_tx.send(ScriptReloadEvent::FullReload);
                        break;
                    }
                }
            }
        }
    }

    Ok(())
}
```

Then wire it up exactly like the Config/Theme pattern: store a `control_tx` that sends `Stop`, and join the thread.

**This fixes:**

* the flush-thread race,
* the flush-thread leak,
* the join hang,
* plus gives you a sane storm strategy.

---

# 5.3 notify crate choice and alternatives

### Is `notify` the right choice?

Yes—especially macOS-first—because it maps to FSEvents and is the standard Rust approach.

The real problem isn’t `notify`. It’s that you’re currently treating raw notify events as if they were clean, de-duplicated “file changed” signals. They’re not.

### Better option *within the notify ecosystem*

Use a debouncer wrapper instead of rolling your own:

* `notify` has companion debouncer crates (commonly used to handle duplicate bursts and rename weirdness).
* If you adopt one, you can delete most of your debounce code.

If you don’t want new deps: implement the “single loop + recv_timeout” pattern above. It’s solid.

### Alternatives

* Platform-specific watchers (macOS FSEvents directly, Linux inotify directly) can reduce quirks but you lose cross-platform and you’ll still need debouncing semantics.
* Tools like watchexec are heavier and not great as an embedded dependency.

Net: stick with `notify`, fix your event pipeline.

---

# 5.4 One watcher vs four watchers

### Should you share a single `notify::Watcher`?

It’s optional.

**Pros**

* fewer OS watcher handles / streams,
* fewer threads,
* centralized dedupe + coalesce,
* easier to implement “if scripts storm, collapse to FullReload” globally.

**Cons**

* script storms can delay config/theme processing if you do everything in one serial loop (unless you classify + prioritize),
* you’ll end up writing routing code anyway.

**Practical recommendation**

* Combine **Config + Theme** into one watcher/service (same directory, same semantics).
* Keep **Scripts** separate if you want isolation from event storms.
* Or go “all-in” on a `HotReloadService` that routes events and enforces storm limits.

Given your “launcher must feel instant” constraint, I’d bias toward **centralizing the logic** but keeping **storm isolation** (scripts separate is fine).

---

# 5.5 Robustness against transient errors

What you want is: “watchers don’t silently die and don’t require a restart to recover.”

Concrete measures:

1. **Supervisor restart with backoff** for setup-time failures
   Wrap your “setup watcher + loop” in an outer loop:

* if setup fails (directory missing, permission transient, etc.),
* log once,
* sleep (exponential backoff),
* retry until stop is requested.

2. **Handle missing directories intentionally**
   You currently skip watching extensions if the directory doesn’t exist. If it appears later, you’ll never watch it.

Options:

* Watch the parent directory and detect creation, then add a watch.
* Or periodically re-check existence (cheap timer) and add watch once.

3. **Storm coalescing**
   Bulk FS operations produce garbage-level event volume on every watcher implementation. Decide your policy:

* If >N events in M ms: send `FullReload` once and ignore the rest until quiet.

4. **Don’t treat notify errors as “log and carry on forever”**
   If you get repeated `Err(e)` from notify, consider counting them and restarting the watcher after a threshold. Otherwise you can get stuck in a loop that never produces real events again.

---

## Direct answers to your 5 “Questions for Expert”

1. **Correct debounce pattern in Rust? Dedicated crate?**
   Use *trailing-edge debounce* implemented in the receiving loop via `recv_timeout` and a moving deadline. It’s the simplest correct approach in sync Rust. If you want to stop thinking about notify quirks, use a notify debouncer wrapper crate and treat its output as “already debounced”.

2. **Watcher thread lifecycle? Implement Drop?**
   Yes, but only if Drop can actually stop the thread. Add a `Stop` control channel or atomic stop flag. Don’t call `join()` without a stop signal, or you’ll hang shutdown.

3. **Is notify right? Alternatives with better dedupe?**
   `notify` is fine. The “dedupe” belongs in your layer (or a notify debouncer wrapper). Platform-specific alternatives exist but just move the complexity around.

4. **Should all watchers share a single notify::Watcher?**
   Not required for resource usage (four is not crazy), but it can simplify routing and dedupe. If you do it, make sure script storms can’t starve config/theme.

5. **Robust against transient errors (permissions, network drives)?**
   Supervisor-restart with backoff + storm coalescing + intentional handling of missing dirs + “restart on repeated notify errors”. For network-drive weirdness, consider an optional polling fallback if needed later.

---

## What I would do first (highest ROI)

1. **Fix shutdown/join** for all watchers (Stop signal + join). This is a correctness landmine.
2. **Replace config/theme debounce** with trailing-edge `recv_timeout` loop and treat rename/remove as relevant.
3. **Delete ScriptWatcher flush thread**, do debouncing in one loop, add storm → `FullReload`.
4. Add a **restart supervisor** if you care about long-running robustness.

If you implement just those four steps, you’ll eliminate the real races, the shutdown deadlock, and most of the notify “duplicate event” pain.
