## Vercel AI Gateway (OpenAI‑compatible) reality check

Based on Vercel’s current docs, the OpenAI‑compatible interface looks like this:

* **Base URL:** `https://ai-gateway.vercel.sh/v1`
* **Chat completions endpoint:** `POST /chat/completions` (so full URL: `https://ai-gateway.vercel.sh/v1/chat/completions`)
* **Auth:** `Authorization: Bearer …` using either an **AI Gateway API key** or a **Vercel OIDC token**
* **Models:** model IDs are **namespaced**, e.g. `anthropic/claude-sonnet-4.5`, `openai/gpt-4.1-mini`
* **Tool/function calling:** supported via OpenAI‑style tool calls
* **Optional headers:** `http-referer` and `x-title` are optional “app attribution” headers (not required for requests to work).
* **Rate limiting:** documented as standard `429 Too Many Requests` with OpenAI‑style error JSON.

So the `https://api.vercel.com/v1/ai-gateway/openai/chat/completions` path you quoted doesn’t match the current “OpenAI‑compatible API” docs. If you stick with your current provider shape (posting to a single URL), you want the full endpoint URL to be:

```txt
https://ai-gateway.vercel.sh/v1/chat/completions
```

…and you should expect model IDs like:

```txt
openai/gpt-4o
anthropic/claude-sonnet-4.5
google/gemini-1.5-pro
```

---

# 1) Critical Issues (bugs / correctness / architectural footguns)

### 1. Blocking network I/O will freeze your UI if this runs on the GPUI thread

Your `AiProvider` trait is synchronous and `OpenAiProvider` uses blocking `ureq`. If any caller invokes `send_message` / `stream_message` on the UI thread, the launcher (or the chat window) will hitch or hard-freeze during DNS/TLS handshake, slow networks, etc. This is the #1 “this will bite you in production” issue.

**Action:** enforce off-main-thread execution (spawn worker thread / task executor) or move to async with cancellation (more below).

---

### 2. No timeouts → hangs forever

You’re using `ureq::post(...).send_json(...)` without configuring connect/read/write timeouts. A stuck TCP connection can hang indefinitely. That’s catastrophic in a desktop app.

**Action:** use a `ureq::Agent` with explicit timeouts and reuse it across requests.

---

### 3. `base_url` semantics are inconsistent with your Vercel plan

`OpenAiProvider::api_url()` returns `config.base_url` **verbatim**, otherwise the hardcoded full URL `https://api.openai.com/v1/chat/completions`.

That means:

* If you set `base_url = "https://ai-gateway.vercel.sh/v1"` (the doc-style base URL), **your code will POST to the wrong place** (`/v1` instead of `/v1/chat/completions`).
* Your proposed Option A/B uses a “base” and expects later path concatenation, but your implementation doesn’t do that.

**Action:** either:

* rename `base_url` to `endpoint_url` and require the full `/chat/completions` URL, **or**
* store a real base URL and join paths properly.

---

### 4. You can’t support tool/function calling with the current data model

Tool calling needs:

* streaming deltas in `delta.tool_calls`
* tool result messages that include `tool_call_id` (and often non-text content types)

Your `ProviderMessage` is `{ role: String, content: String }`. That cannot represent tool calls or tool results.

Also, `parse_sse_line` only extracts `delta.content`, so tool-call deltas are silently dropped.

**Action:** introduce a structured message/content model and a structured streaming event type (details below). Otherwise “function calling supported” is only true on paper.

---

### 5. Streaming end-of-stream handling is too weak

You treat `data: [DONE]` as `None` (same as “no delta”), so the read loop never breaks early. You rely on the server to close the connection promptly.

That usually works, until it doesn’t (keep-alives, intermediaries, proxy behavior). When it doesn’t, your stream may hang after completion.

**Action:** detect `[DONE]` and break the loop.

---

### 6. Vercel provider_id collisions if you “modify OpenAiProvider”

Your registry is a `HashMap<String, Arc<dyn AiProvider>>` keyed by `provider_id()`. If you implement “OpenAI via Vercel” as provider_id `"openai"`, you will overwrite the real OpenAI provider (or vice-versa).

**Action:** Vercel gateway must have a distinct `provider_id` (e.g. `"vercel_gateway"`).

---

# 2) Performance Concerns (latency, memory, throughput)

### 1. No connection reuse

If you construct requests with `ureq::post(...)` without a shared `Agent`, you may not get ideal connection pooling behavior. For a chat window, this shows up as extra latency on every request (TLS handshakes, etc).

**Action:** put a `ureq::Agent` inside each provider and reuse it.

---

### 2. `BufRead::lines()` allocates per line and loses SSE multi-line semantics

SSE can legally send an event as multiple `data:` lines. OpenAI-style streams usually don’t, but gateways/proxies sometimes do.

Your loop:

* reads line-by-line
* ignores non-`data:` lines
* doesn’t buffer multi-line events

**Action:** implement a minimal SSE event accumulator (buffer `data:` lines until blank separator), then parse the full JSON event.

---

### 3. `available_models()` is hardcoded and will fall out of date (especially for gateway)

For Vercel gateway, models change frequently and are **namespaced**. The docs explicitly support `GET /models`.

**Action:** either (a) fetch models lazily and cache, or (b) accept “user types model id” and keep only a small curated list.

But don’t pretend static lists are “the” model list for a gateway.

---

# 3) API Design Feedback (patterns, abstractions, future-proofing)

### 1. Split “OpenAI-compatible protocol” from “provider identity”

Right now, OpenAI and “OpenAI-compatible” are conflated. Vercel is OpenAI-compatible, but not OpenAI. Architecturally you want:

* **Protocol adapter:** “OpenAI chat completions protocol” (request/response/stream parser)
* **Provider configuration:** base URL, auth header, model ID normalization, optional attribution headers

This avoids copying OpenAI logic into a second provider.

---

### 2. Make streaming event-based, not “string chunks only”

Right now, you only stream text deltas. Real streams include:

* role deltas
* tool call deltas
* usage (sometimes)
* provider metadata (Vercel mentions provider metadata in streaming chunks in examples)

Suggested shape:

```rust
pub enum StreamEvent {
    Text(String),
    ToolCallDelta(ToolCallDelta),
    Done,
    Error(String),
    // maybe: Usage(Usage), Metadata(Value)
}
```

Then your UI can decide what to do. Your existing `StreamCallback = Fn(String)` can be kept for backward compatibility as a “text-only adapter.”

---

### 3. Stop using free-form `role: String` in core types

Use an enum for roles to prevent invalid values leaking to APIs:

```rust
enum Role { System, User, Assistant, Tool }
```

You can still accept strings at the boundary for backwards compatibility, but validate/normalize once.

---

# 4) Simplification Opportunities (remove duplication / overengineering)

### 1. Extract the SSE read loop

Both OpenAI and Anthropic duplicate the same “read SSE lines, skip blanks, parse” logic.

Make a reusable helper:

```rust
fn read_sse<R: std::io::Read>(
    reader: R,
    mut on_data: impl FnMut(&str) -> anyhow::Result<SseAction>,
) -> anyhow::Result<()>
```

Where `SseAction` can be `Continue | Break`.

---

### 2. Create a generic `OpenAiCompatProvider`

Then:

* `OpenAiProvider` becomes “OpenAI endpoint + OpenAI key”
* `VercelGatewayProvider` becomes “Vercel endpoint + AI_GATEWAY_API_KEY + model prefix mapping”

Same code path, fewer bugs.

---

# 5) Specific Recommendations (concrete changes + examples)

Below are changes I’d actually make next.

---

## A) Fix URL handling properly (and make Vercel integration not fragile)

### Step 1: store a base URL, not a full endpoint (or support both)

```rust
const OPENAI_BASE_URL: &str = "https://api.openai.com/v1";
const CHAT_COMPLETIONS_PATH: &str = "/chat/completions";

fn join_url(base: &str, path: &str) -> String {
    // simple, avoids double slashes
    let base = base.trim_end_matches('/');
    let path = path.trim_start_matches('/');
    format!("{}/{}", base, path)
}
```

Then in OpenAI provider:

```rust
fn base_url(&self) -> &str {
    self.config.base_url.as_deref().unwrap_or(OPENAI_BASE_URL)
}

fn chat_completions_url(&self) -> String {
    // if caller passed a full endpoint URL already, keep it working:
    let base = self.base_url();
    if base.ends_with("/chat/completions") {
        base.to_string()
    } else {
        join_url(base, CHAT_COMPLETIONS_PATH)
    }
}
```

**Why:** this lets you set Vercel base URL exactly as documented (`https://ai-gateway.vercel.sh/v1`)  and still work.

---

## B) Add a real Vercel gateway provider (don’t overload OpenAI provider_id)

### Minimal provider wrapper (reuses OpenAI-compatible code)

```rust
pub struct VercelGatewayProvider {
    inner: OpenAiCompatibleProvider,
}

impl VercelGatewayProvider {
    pub fn new(api_key: impl Into<String>) -> Self {
        let inner = OpenAiCompatibleProvider::new(
            ProviderConfig::new("vercel_gateway", "Vercel AI Gateway", api_key),
            "https://ai-gateway.vercel.sh/v1",
        )
        // Back-compat: if user passes "gpt-4o" normalize to "openai/gpt-4o"
        .with_default_model_namespace("openai");
        Self { inner }
    }
}
```

### Key model mapping detail

Vercel model IDs are namespaced (`openai/...`, `anthropic/...`).
To keep backwards compatibility with Script Kit scripts that pass `"gpt-4o"`, do:

```rust
fn normalize_model_id(&self, model_id: &str) -> std::borrow::Cow<'_, str> {
    if self.default_model_namespace.is_some() && !model_id.contains('/') {
        std::borrow::Cow::Owned(format!("{}/{}", self.default_model_namespace.unwrap(), model_id))
    } else {
        std::borrow::Cow::Borrowed(model_id)
    }
}
```

---

## C) Timeouts + connection reuse (do this immediately)

```rust
pub struct OpenAiCompatibleProvider {
    config: ProviderConfig,
    base_url: String,
    agent: ureq::Agent,
    default_model_namespace: Option<&'static str>,
    extra_headers: Vec<(String, String)>,
}

impl OpenAiCompatibleProvider {
    pub fn new(config: ProviderConfig, base_url: impl Into<String>) -> Self {
        let agent = ureq::AgentBuilder::new()
            .timeout_connect(std::time::Duration::from_secs(10))
            .timeout_read(std::time::Duration::from_secs(60))
            .timeout_write(std::time::Duration::from_secs(60))
            .build();

        Self {
            config,
            base_url: base_url.into(),
            agent,
            default_model_namespace: None,
            extra_headers: vec![],
        }
    }
}
```

For streaming, you may want a larger read timeout or a “no progress” timeout strategy, but **do not** leave it unbounded.

---

## D) Make streaming robust (break on DONE + handle multi-line SSE)

Replace line-by-line parsing with event buffering:

```rust
fn stream_sse(reader: impl std::io::Read, mut on_event: impl FnMut(&str) -> anyhow::Result<bool>) -> anyhow::Result<()> {
    let mut reader = std::io::BufReader::new(reader);
    let mut buf = String::new();
    let mut data_lines: Vec<String> = Vec::new();

    while reader.read_line(&mut buf)? != 0 {
        let line = buf.trim_end_matches(&['\r', '\n'][..]).to_string();
        buf.clear();

        if line.is_empty() {
            // end of SSE event
            if !data_lines.is_empty() {
                let data = data_lines.join("\n");
                data_lines.clear();

                if data == "[DONE]" {
                    break;
                }

                // on_event returns true to continue, false to stop
                if !on_event(&data)? {
                    break;
                }
            }
            continue;
        }

        if let Some(rest) = line.strip_prefix("data: ") {
            data_lines.push(rest.to_string());
        }
    }

    Ok(())
}
```

Then your OpenAI-compatible streaming becomes: parse full JSON event; extract `delta.content` and send text chunks.

---

## E) Upgrade error reporting (right now you lose the useful part)

When ureq returns status errors, capture body:

```rust
fn send_json(req: ureq::Request, body: &serde_json::Value) -> anyhow::Result<ureq::Response> {
    match req.send_json(body) {
        Ok(resp) => Ok(resp),
        Err(ureq::Error::Status(code, resp)) => {
            let text = resp.into_body().read_to_string().unwrap_or_default();
            anyhow::bail!("HTTP {code}: {text}");
        }
        Err(e) => Err(anyhow::anyhow!(e)),
    }
}
```

Do the same for streaming requests. This is how you’ll surface gateway quota issues, 401s, etc without guessing.

Vercel documents OpenAI-style error JSON payloads.

---

## F) Tool calling support: don’t fake it — add the types

If you want “function calling supported,” the minimum is:

1. represent tool calls in `ProviderMessage`
2. parse tool call deltas in streaming

A low-friction step that doesn’t break existing call sites:

* keep your existing `send_message()` / `stream_message()` returning/streaming **text**
* add an **optional** new method:

```rust
pub trait AiProvider {
    // existing...
    fn stream_message(&self, ...);

    // new, defaulted:
    fn stream_events(&self, messages: &[ProviderMessage], model_id: &str, on_event: Box<dyn Fn(StreamEvent) + Send + Sync>) -> Result<()> {
        // default implementation calls stream_message and maps Text(...)
        self.stream_message(messages, model_id, Box::new(move |chunk| on_event(StreamEvent::Text(chunk))))
    }
}
```

Then OpenAI-compatible provider can override `stream_events` to emit `ToolCallDelta` too.

Vercel explicitly supports “tool calls” on the OpenAI-compatible chat completions endpoint.

---

## G) Vercel gateway headers: add optional “app attribution”

Not required, but easy to add and harmless.

Vercel says `http-referer` + `x-title` are optional.

So expose them in config and include them if present.

---

## H) Model list: don’t hardcode the gateway’s universe

Vercel supports `GET /models` for the OpenAI-compatible API.

If you want a dynamic list without blocking UI:

* keep `available_models()` returning a small static “popular models” list immediately
* add a background refresh that fills a cache from `GET /models`

If you keep it sync-only, at least provide a “type any model id” escape hatch, because the gateway’s model list will always move faster than your releases.

---

## I) Integration tests you should add

* URL join test: base URL + path, no double slashes
* Vercel model normalization: `"gpt-4o"` → `"openai/gpt-4o"`, but `"anthropic/..."` unchanged
* SSE DONE handling: stops loop on `[DONE]`
* “multi-line SSE event” test: data lines joined into one JSON string

And add ignored “system tests” that hit the gateway if `SCRIPT_KIT_AI_GATEWAY_API_KEY` is set.

---

## Summary: what I’d do next (ordered)

1. **Add timeouts + shared ureq::Agent** (stability)
2. **Fix URL/base_url semantics** (unblocks Vercel integration cleanly)
3. **Create `VercelGatewayProvider` with unique provider_id** (registry correctness)
4. **Normalize model IDs for backwards compatibility** (scripts keep working)
5. **Harden SSE parsing + break on DONE** (avoid hanging streams)
6. **Plan tool-calling support by extending message + stream event model** (stop pretending it’s supported when it isn’t)

---

```text
URLs
- https://vercel.com/docs/ai-gateway/openai-compat
- https://vercel.com/docs/ai-gateway/app-attribution
- https://vercel.com/docs/llms-full.txt
```
