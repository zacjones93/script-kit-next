## 1) Critical issues

### A. **API keys will leak via `Debug`**

Both `ProviderConfig` and `DetectedKeys` derive `Debug`, but they contain raw API keys (`api_key: String`, `Option<String>`). Any `{:?}` log, panic, error context, or “dump state” debug UI will print secrets.

**Why this is critical:** your own docs say keys “should never be logged or displayed” — right now the type system actively makes leaks easy.

**Fix (recommended):**

* Stop deriving `Debug` for anything that contains secrets, **or**
* Store secrets in a “secret” type + implement a redacting `Debug`.

Concrete example (minimal impact):

```rust
use std::fmt;

#[derive(Clone)]
pub struct ProviderConfig {
    pub provider_id: String,
    pub display_name: String,
    api_key: String,
    pub base_url: Option<String>,
}

impl fmt::Debug for ProviderConfig {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.debug_struct("ProviderConfig")
            .field("provider_id", &self.provider_id)
            .field("display_name", &self.display_name)
            .field("api_key", &"<redacted>")
            .field("base_url", &self.base_url)
            .finish()
    }
}
```

Do the same for `DetectedKeys` (or don’t implement `Debug` at all).

If you’re open to a dependency, `secrecy::SecretString` makes this much harder to screw up long-term.

---

### B. **Cloning secrets is too easy**

`ProviderConfig: Clone` will duplicate the API key string each time. That increases the chance of accidental exposure and increases memory lifetime of the secret.

**Fix options:**

* Store the key as `Arc<str>` or a secret wrapper so clones are cheap and controlled.
* Or remove `Clone` from `ProviderConfig` and pass configs by `Arc<ProviderConfig>`.

---

### C. **You’re setting yourself up for model-ID incompatibility**

Right now your built-in model ids are provider-local (`"gpt-4o"`, `"claude-3-5-sonnet-..."`). Vercel AI Gateway model ids are typically **provider-qualified** like `"openai/gpt-5"` or `"anthropic/claude-sonnet-4.5"`.

If you introduce gateway models without a normalization layer, you’ll end up with:

* duplicates (“gpt-4o” vs “openai/gpt-4o”),
* confusing UX (“why does the same model appear twice?”),
* and bugs when scripts/providers expect one format.

**Fix (required for backwards compat):**
Add a single canonical representation (even if you still *accept* legacy ids).

Example strategy:

* Canonical id: `provider/model` when provider is known.
* Accept legacy ids by mapping:

  * if `id` contains `/`, treat it as canonical.
  * else use the selected provider to qualify it (e.g. `openai + gpt-4o -> openai/gpt-4o`).

---

## 2) Performance concerns

### A. `default_models::*()` reallocates every call

Every call builds a new `Vec<ModelInfo>`. If the model picker rebuilds often (search/filter re-renders), you’ll churn allocations.

**Fix:** convert these to `const` slices + `to_vec()` only when you truly need ownership, or store them in `OnceLock`.

```rust
use std::sync::OnceLock;

static OPENAI_DEFAULTS: OnceLock<Vec<ModelInfo>> = OnceLock::new();

pub fn openai() -> &'static [ModelInfo] {
    OPENAI_DEFAULTS.get_or_init(|| vec![
        ModelInfo::new("gpt-4o", "GPT-4o", "openai", true, 128_000),
        // ...
    ])
}
```

(If you need a `Vec`, clone once at the boundary.)

---

### B. Model discovery must not block UI / render

Vercel’s `/models` list can be **hundreds** of entries. Parsing + grouping + sorting that inside a paint cycle is a stutter factory.

**Fix pattern:**

* Load cached models synchronously (fast).
* Fetch remote models asynchronously.
* Precompute derived UI structures (category buckets, lowercase search keys) **once** per refresh, not per keystroke.

---

## 3) API design feedback

### A. Separate “credentials/config” from “catalog/models”

Right now `config.rs` mixes:

* provider key discovery,
* model definitions,
* model metadata concerns.

Once you add dynamic discovery + caching, you’ll want a clean split:

* `ProviderConfig` / `DetectedKeys`: secrets + endpoints + auth.
* `ModelCatalog` service: “give me models for provider X”, with caching and refresh.
* `ModelInfo` / `ModelMeta`: immutable records.

This keeps your launcher fast and avoids coupling “read env vars” with “fetch network”.

---

### B. Make streaming + context window tri-state (or explicitly “unknown”)

Your current `ModelInfo` forces:

* `supports_streaming: bool`
* `context_window: u32`

But gateway discovery may return models where:

* you don’t know streaming support (or it can vary),
* context window might be missing/unknown.

Vercel’s model list can include `context_window`, `max_tokens`, `type`, `tags`, and `pricing`, but you should still code defensively because fields can vary over time.

**Recommendation:**

* Change to `supports_streaming: Option<bool>`
* Change to `context_window: Option<u32>`

If you can’t change the struct broadly, adopt a convention like `context_window = 0` == unknown, but that’s a leaky hack.

---

### C. Prefer newtypes for ids you’ll use everywhere

You’re about to lean heavily on:

* provider IDs
* model IDs

Stringly-typed identifiers are how you get subtle bugs.

At minimum:

```rust
#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub struct ProviderId(pub String);

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub struct ModelId(pub String); // canonical: "provider/model"
```

Then add helpers: `provider()`, `name()`, `qualify(provider, legacy_id)`.

---

## 4) Simplification opportunities

### A. Replace `DetectedKeys { openai: Option<String>, ... }` with a map

If you keep adding providers (OpenRouter, Vercel, xAI, etc.), the struct becomes boilerplate-heavy.

A clean alternative:

```rust
use std::collections::HashMap;

pub struct DetectedKeys {
    pub keys: HashMap<String, String>, // or SecretString
}
```

You can still provide convenience accessors for “known” providers if needed.

---

### B. Reduce duplication in default model lists

The default lists are fine as a fallback, but you can make them less error-prone by:

* storing a single array of tuples,
* mapping into `ModelInfo` in one place.

This also makes it easier to keep defaults aligned with your “featured” list.

---

## 5) Specific recommendations (concrete code + Vercel integration)

### A. Use the **AI Gateway OpenAI-compatible** `/models` endpoint (not `api.vercel.com/v1/ai-gateway/models`)

Vercel’s docs for the OpenAI-compatible API specify:

* Base URL: `https://ai-gateway.vercel.sh/v1`
* List models: `GET /models`
* Retrieve model: `GET /models/{model}`

So your fetch should be against:

```text
GET https://ai-gateway.vercel.sh/v1/models
Authorization: Bearer <AI_GATEWAY_API_KEY or VERCEL_OIDC_TOKEN>
```

This directly answers your Key Question #1: **Yes, there is a `/models` endpoint**, and it’s the OpenAI-compatible one.

---

### B. Parse Vercel model metadata defensively

Vercel’s model list can include fields like `name`, `description`, `context_window`, `max_tokens`, `type`, capability `tags`, and `pricing`.

Implement it as optional fields so schema drift doesn’t break you:

```rust
use serde::Deserialize;

#[derive(Debug, Deserialize)]
pub struct GatewayModelsResponse {
    pub object: String,
    pub data: Vec<GatewayModel>,
}

#[derive(Debug, Deserialize)]
pub struct GatewayModel {
    pub id: String,
    pub object: Option<String>,
    pub created: Option<i64>,
    pub owned_by: Option<String>,

    // Extended metadata (may be present)
    pub name: Option<String>,
    pub description: Option<String>,
    pub context_window: Option<u32>,
    pub max_tokens: Option<u32>,

    #[serde(rename = "type")]
    pub model_type: Option<String>, // "language" | "embedding" | "image"

    pub tags: Option<Vec<String>>,
    pub pricing: Option<serde_json::Value>, // structure varies, keep flexible
}
```

Then map into your internal type:

```rust
impl From<GatewayModel> for ModelInfo {
    fn from(m: GatewayModel) -> Self {
        let provider = m.id.split('/').next().unwrap_or("gateway").to_string();
        let display = m.name.clone().unwrap_or_else(|| m.id.clone());

        ModelInfo::new(
            m.id,
            display,
            provider,
            /* supports_streaming */ m.model_type.as_deref() == Some("language"),
            m.context_window.unwrap_or(0),
        )
    }
}
```

(If you adopt `Option<u32>` for context, don’t use the `0` hack.)

---

### C. Caching strategy that won’t hurt “instant launcher”

You asked “how often should we refresh” and “offline fallback”.

Here’s the strategy that fits your constraints (instant open, 60fps, multi-window):

1. **On app start**

   * Load cached models from disk/SQLite immediately (fast).
   * If cache is older than TTL, kick off background refresh.

2. **TTL**

   * Default: **24 hours**.
   * Also refresh on explicit “Refresh” action in the UI.

3. **On opening model picker**

   * Never block UI waiting on network.
   * If refresh is running, show a tiny spinner + “Updating…” but keep list usable.

4. **Offline fallback**

   * Use cached list if present.
   * If no cache exists, fall back to your `default_models` hardcoded list and show a one-line warning (non-modal).

This answers Key Questions #2 and #3 with an approach that prioritizes responsiveness.

---

### D. “Featured models” should be a *curated overlay*, not a category computed from remote

Your proposed `ModelCategory::Featured` is good — but don’t try to infer “featured” from the remote list. Keep a curated list of ids you want to pin (and only display those if they exist in the fetched set).

Example:

```rust
const FEATURED: &[&str] = &[
    "openai/gpt-5",
    "openai/gpt-4o",
    "anthropic/claude-sonnet-4.5",
    // ...
];
```

Then filter `models` to those ids.

This keeps UX stable even when the remote catalog changes.

---

### E. Aliases like `best` / `latest`: implement *client-side*, and optionally use gateway fallback lists

I don’t see Vercel docs describing first-class “`best` / `latest`” aliases for model ids. What they *do* support is **model fallback lists** via a `models` array (gateway extension) or via provider options, which lets you try “best-first” in an ordered list.

So the clean approach:

* In Script Kit: define aliases → expand to an ordered list of concrete model ids.
* When using AI Gateway: send the list using the gateway’s fallback mechanism.

That gives you “best” semantics without relying on undocumented server aliases.

---

### F. Add environment variable support for AI Gateway without breaking your `SCRIPT_KIT_*` security posture

You can keep your prefix AND still be ergonomic:

* Preferred: `SCRIPT_KIT_AI_GATEWAY_API_KEY`
* Fallback read (optional): `AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN` (useful for users already set up per Vercel docs).

That reduces friction while keeping your “explicit Script Kit” convention.

---

## URLs

```text
https://vercel.com/docs/ai-gateway/openai-compat
https://vercel.com/docs/ai-gateway/models-and-providers
https://vercel.com/docs/ai-gateway/provider-options
https://vercel.com/docs/llms-full.txt
```
